{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:40:48.753180Z",
     "start_time": "2020-03-24T06:40:48.735229Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Reshape, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Add, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import he_normal, constant\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical, normalize\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:33:54.915443Z",
     "start_time": "2020-03-24T03:33:54.538451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avi_train_seg = np.load('../Data/avi_train_seg.npy')\n",
    "trj_train_seg = np.load('../Data/trj_train_seg.npy')\n",
    "lane_functions = np.load('../Data/lane_functions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:33:54.937384Z",
     "start_time": "2020-03-24T03:33:54.926414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_lanes = lane_functions.sum(axis=1)\n",
    "lane_functions = pd.DataFrame(lane_functions,columns=['through','left','right',\n",
    "                                                      'thr_left','thr_right','u_turn'])\n",
    "lane_functions['linkIdx'] = np.arange(1,25)\n",
    "lane_functions['num_lanes'] = num_lanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:33:56.925066Z",
     "start_time": "2020-03-24T03:33:56.917088Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sample(trj_train_seg):\n",
    "    data = pd.DataFrame(columns=['linkIdx','datetime','volume'])\n",
    "    for approach in range(24):\n",
    "        table = pd.DataFrame(np.zeros((30 * 144,3)),columns=['linkIdx','datetime','volume'])\n",
    "        table.iloc[:,0] = approach + 1\n",
    "        for day in range(30):\n",
    "            if day < 9:\n",
    "                datetime = [pd.to_datetime('2018010' + str(day + 1)) + pd.Timedelta(i * 10,unit='m') for i in range(144)]\n",
    "            else:\n",
    "                datetime = [pd.to_datetime('201801' + str(day + 1)) + pd.Timedelta(i * 10,unit='m') for i in range(144)]\n",
    "            table.iloc[144 * day:144 * (day + 1),2] = trj_train_seg[day,:,approach]\n",
    "            table.iloc[144 * day:144 * (day + 1),1] = datetime\n",
    "        data = pd.concat((data,table))\n",
    "    data['linkIdx'] = data['linkIdx'].astype('int')\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    data['volume'] = data['volume'].astype('float')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:13:37.640732Z",
     "start_time": "2020-03-24T05:13:09.293567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Basic Stats] num. of samples: 103680\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_sample(trj_train_seg), get_sample(avi_train_seg)\n",
    "print(\"[Basic Stats] num. of samples: {:d}\".format(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:34:26.527900Z",
     "start_time": "2020-03-24T03:34:26.510946Z"
    }
   },
   "outputs": [],
   "source": [
    "def holiday(x):\n",
    "    if x.month == 1 and x.day == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def peak(x):\n",
    "    if 7 < x.hour < 9:\n",
    "        return 1\n",
    "    elif 11 < x.hour < 13:\n",
    "        return 2\n",
    "    elif 17 < x.hour < 19:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def itv_cnt(x, itv_length=600):\n",
    "    return (x.hour * 3600 + x.minute * 60) // itv_length\n",
    "\n",
    "\n",
    "def exponential_smoothing(alpha, s):\n",
    "    s2 = np.zeros(s.shape)\n",
    "    s2[0] = s[0]\n",
    "    for i in range(1, len(s2)):\n",
    "        s2[i] = alpha * s[i] + (1 - alpha) * s2[i - 1]\n",
    "    return s2\n",
    "\n",
    "\n",
    "def get_es_volume(trj_train_seg, alpha):\n",
    "    trj_train_seg_es = trj_train_seg.copy()\n",
    "    for day_idx in range(30):\n",
    "        for seg_idx in range(24):\n",
    "            seq = trj_train_seg[day_idx, :, seg_idx]\n",
    "            seq_es = exponential_smoothing(alpha, seq)\n",
    "            trj_train_seg_es[day_idx, :, seg_idx] = seq_es\n",
    "    return get_sample(trj_train_seg_es)['volume']\n",
    "\n",
    "\n",
    "def merge_volume_features(X, feature_cols, target_col, aggfuncs=['mean', 'median', 'std']):\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        for fn in aggfuncs:\n",
    "            df = X.pivot_table(index='linkIdx',\n",
    "                               columns=feature,\n",
    "                               values=target_col,\n",
    "                               aggfunc=fn).reset_index()\n",
    "            df.columns = ['linkIdx'] + list(df.columns[1:])\n",
    "            df = df.melt(id_vars=['linkIdx'],\n",
    "                         value_vars=list(df.columns[1:]),\n",
    "                         var_name=feature,\n",
    "                         value_name=feature + '_' + fn + '_' + target_col)\n",
    "            df[feature] = pd.to_numeric(df[feature])\n",
    "            X = pd.merge(X, df, on=['linkIdx',feature])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:10.620296Z",
     "start_time": "2020-03-24T05:13:43.534762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Basic Stats] dim. of features: 20\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Feature Engineering\n",
    "# =============================\n",
    "\n",
    "# static attributes\n",
    "'''\n",
    "X = pd.merge(X,lane_functions)\n",
    "'''\n",
    "X['weekday'] = X['datetime'].map(lambda x:x.weekday())\n",
    "X['interval'] = X['datetime'].map(lambda x:itv_cnt(x))\n",
    "X['holiday'] = X['datetime'].map(lambda x:holiday(x))\n",
    "X['peak'] = X['datetime'].map(lambda x:peak(x))\n",
    "X['linkIdx'] = X['linkIdx'].astype(int)\n",
    "\n",
    "# exponenrially smoothed volume\n",
    "X['volume_es_p6'] = get_es_volume(trj_train_seg, alpha=0.6).values\n",
    "X['volume_es_p5'] = get_es_volume(trj_train_seg, alpha=0.5).values\n",
    "\n",
    "# cross volume features\n",
    "'''\n",
    "feature_cols = ['through', 'left', 'right', 'thr_left',\n",
    "                'thr_right', 'u_turn', 'num_lanes',\n",
    "                'weekday', 'interval', 'holiday', 'peak']\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume')\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume_es_p6')\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume_es_p5')\n",
    "'''\n",
    "# penetration rates\n",
    "X['tmp'] = Y['volume'].values\n",
    "interval_volume = X.pivot_table(index='interval',\n",
    "                                values=['volume', 'tmp',\n",
    "                                        'volume_es_p6', 'volume_es_p5'],\n",
    "                                aggfunc='sum').reset_index()\n",
    "interval_volume['penetration'] = interval_volume['volume'] / interval_volume['tmp']\n",
    "interval_volume['penetration_p6'] = interval_volume['volume_es_p6'] / interval_volume['tmp']\n",
    "interval_volume['penetration_p6'] = interval_volume['volume_es_p5'] / interval_volume['tmp']\n",
    "interval_volume.drop(['tmp','volume','volume_es_p5','volume_es_p6'],axis=1,inplace=True)\n",
    "X = pd.merge(X, interval_volume, on='interval')\n",
    "\n",
    "# scaled volume\n",
    "for up_idx, up_col in enumerate(['volume','volume_es_p6','volume_es_p5']):\n",
    "    for down_idx, down_col in enumerate(['penetration','penetration_p6','penetration_p6']):\n",
    "        X['scaled_volume_' + str(up_idx) + '_' + str(down_idx)] = X[up_col] / X[down_col]\n",
    "\n",
    "# correlations & feature count\n",
    "'''\n",
    "X.corr().to_excel('../Data/feature_corr.xls')\n",
    "'''\n",
    "X.drop('tmp',axis=1,inplace=True)\n",
    "print(\"[Basic Stats] dim. of features: {:d}\".format(X.shape[1]))\n",
    "\n",
    "# cat & num features\n",
    "categorical_features = ['interval', 'weekday', 'holiday', 'peak']\n",
    "numeric_features = list(set(X.drop(['datetime', 'linkIdx'], axis=1).columns) - set(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:15:41.354638Z",
     "start_time": "2020-03-24T05:15:41.344664Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_split(X, Y, split_mode):\n",
    "    if split_mode is 'random':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.drop(['datetime','linkIdx'], axis=1),\n",
    "                                                            Y['volume'],\n",
    "                                                            test_size=0.2, random_state=2020)\n",
    "    elif split_mode is 'fix_transfer':\n",
    "        train_idx = X[X['linkIdx'].isin([1,2,3,4,5,6,7,8,24,23,22,21,20,19,18,17,15,16])].index\n",
    "        test_idx = X[X['linkIdx'].isin([9,10,11,12,13,14])].index\n",
    "        X_train, X_test = X.drop(['linkIdx', 'datetime'], axis=1).iloc[train_idx, :], X.drop(\n",
    "            ['linkIdx', 'datetime'], axis=1).iloc[test_idx, :]\n",
    "        y_train, y_test = Y['volume'].iloc[train_idx], Y['volume'].iloc[test_idx]\n",
    "        \n",
    "    elif split_mode is 'random_transfer':\n",
    "        test_ls = np.unique(np.random.choice(np.arange(1,25),6))\n",
    "        train_ls = list(set(np.arange(1,25)) - set(test_ls))\n",
    "        train_idx = X[X['linkIdx'].isin(train_ls)].index\n",
    "        test_idx = X[X['linkIdx'].isin(test_ls)].index\n",
    "        X_train, X_test = X.drop(['linkIdx', 'datetime'], axis=1).iloc[train_idx, :], X.drop(\n",
    "            ['linkIdx', 'datetime'], axis=1).iloc[test_idx, :]\n",
    "        y_train, y_test = Y['volume'].iloc[train_idx], Y['volume'].iloc[test_idx]\n",
    "        print(\"Train links: {:s} | Test links: {:s}\".format(str(train_ls), str(test_ls)))\n",
    "    else:\n",
    "        print(\"split mode is wrong !\")\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:35:20.491588Z",
     "start_time": "2020-03-24T03:35:20.486602Z"
    }
   },
   "outputs": [],
   "source": [
    "def mae(y_pred, y_test):\n",
    "    return np.sum(np.abs(y_pred - y_test) * y_test) / np.sum(y_test)\n",
    "\n",
    "def mape(y_pred, y_test):\n",
    "    return np.sum(np.abs(y_pred - y_test)) / np.sum(y_test)\n",
    "\n",
    "def mspe(y_pred, y_test):\n",
    "    return np.sum(np.square(y_pred - y_test)) / np.sum(np.square(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Distribution EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T04:00:43.228783Z",
     "start_time": "2020-03-24T04:00:42.861764Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train links: [1, 3, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 22, 24] | Test links: [ 2  4 12 14 21 23]\n",
      "[volume entropy] inf\n",
      "[through entropy] 0.23\n",
      "[left entropy] 0.30\n",
      "[right entropy] inf\n",
      "[thr_left entropy] nan\n",
      "[thr_right entropy] inf\n",
      "[u_turn entropy] 1.10\n",
      "[num_lanes entropy] 0.09\n",
      "[weekday entropy] 0.00\n",
      "[interval entropy] 0.05\n",
      "[holiday entropy] 0.00\n",
      "[peak entropy] nan\n",
      "[volume_es_p6 entropy] inf\n",
      "[volume_es_p5 entropy] inf\n",
      "[through_mean_volume entropy] 0.32\n",
      "[through_median_volume entropy] 0.44\n",
      "[through_std_volume entropy] 0.21\n",
      "[left_mean_volume entropy] 0.32\n",
      "[left_median_volume entropy] 0.44\n",
      "[left_std_volume entropy] 0.21\n",
      "[right_mean_volume entropy] 0.32\n",
      "[right_median_volume entropy] 0.44\n",
      "[right_std_volume entropy] 0.21\n",
      "[thr_left_mean_volume entropy] 0.32\n",
      "[thr_left_median_volume entropy] 0.44\n",
      "[thr_left_std_volume entropy] 0.21\n",
      "[thr_right_mean_volume entropy] 0.32\n",
      "[thr_right_median_volume entropy] 0.44\n",
      "[thr_right_std_volume entropy] 0.21\n",
      "[u_turn_mean_volume entropy] 0.32\n",
      "[u_turn_median_volume entropy] 0.44\n",
      "[u_turn_std_volume entropy] 0.21\n",
      "[num_lanes_mean_volume entropy] 0.32\n",
      "[num_lanes_median_volume entropy] 0.44\n",
      "[num_lanes_std_volume entropy] 0.21\n",
      "[weekday_mean_volume entropy] 0.32\n",
      "[weekday_median_volume entropy] 0.51\n",
      "[weekday_std_volume entropy] 0.21\n",
      "[interval_mean_volume entropy] inf\n",
      "[interval_median_volume entropy] inf\n",
      "[interval_std_volume entropy] inf\n",
      "[holiday_mean_volume entropy] 0.32\n",
      "[holiday_median_volume entropy] 0.45\n",
      "[holiday_std_volume entropy] 0.21\n",
      "[peak_mean_volume entropy] 0.34\n",
      "[peak_median_volume entropy] 0.61\n",
      "[peak_std_volume entropy] 0.22\n",
      "[through_mean_volume_es_p6 entropy] 0.32\n",
      "[through_median_volume_es_p6 entropy] 0.47\n",
      "[through_std_volume_es_p6 entropy] 0.25\n",
      "[left_mean_volume_es_p6 entropy] 0.32\n",
      "[left_median_volume_es_p6 entropy] 0.47\n",
      "[left_std_volume_es_p6 entropy] 0.25\n",
      "[right_mean_volume_es_p6 entropy] 0.32\n",
      "[right_median_volume_es_p6 entropy] 0.47\n",
      "[right_std_volume_es_p6 entropy] 0.25\n",
      "[thr_left_mean_volume_es_p6 entropy] 0.32\n",
      "[thr_left_median_volume_es_p6 entropy] 0.47\n",
      "[thr_left_std_volume_es_p6 entropy] 0.25\n",
      "[thr_right_mean_volume_es_p6 entropy] 0.32\n",
      "[thr_right_median_volume_es_p6 entropy] 0.47\n",
      "[thr_right_std_volume_es_p6 entropy] 0.25\n",
      "[u_turn_mean_volume_es_p6 entropy] 0.32\n",
      "[u_turn_median_volume_es_p6 entropy] 0.47\n",
      "[u_turn_std_volume_es_p6 entropy] 0.25\n",
      "[num_lanes_mean_volume_es_p6 entropy] 0.32\n",
      "[num_lanes_median_volume_es_p6 entropy] 0.47\n",
      "[num_lanes_std_volume_es_p6 entropy] 0.25\n",
      "[weekday_mean_volume_es_p6 entropy] 0.32\n",
      "[weekday_median_volume_es_p6 entropy] 0.48\n",
      "[weekday_std_volume_es_p6 entropy] 0.25\n",
      "[interval_mean_volume_es_p6 entropy] 1.41\n",
      "[interval_median_volume_es_p6 entropy] 2.09\n",
      "[interval_std_volume_es_p6 entropy] 0.55\n",
      "[holiday_mean_volume_es_p6 entropy] 0.32\n",
      "[holiday_median_volume_es_p6 entropy] 0.47\n",
      "[holiday_std_volume_es_p6 entropy] 0.25\n",
      "[peak_mean_volume_es_p6 entropy] 0.34\n",
      "[peak_median_volume_es_p6 entropy] 0.50\n",
      "[peak_std_volume_es_p6 entropy] 0.26\n",
      "[through_mean_volume_es_p5 entropy] 0.32\n",
      "[through_median_volume_es_p5 entropy] 0.47\n",
      "[through_std_volume_es_p5 entropy] 0.25\n",
      "[left_mean_volume_es_p5 entropy] 0.32\n",
      "[left_median_volume_es_p5 entropy] 0.47\n",
      "[left_std_volume_es_p5 entropy] 0.25\n",
      "[right_mean_volume_es_p5 entropy] 0.32\n",
      "[right_median_volume_es_p5 entropy] 0.47\n",
      "[right_std_volume_es_p5 entropy] 0.25\n",
      "[thr_left_mean_volume_es_p5 entropy] 0.32\n",
      "[thr_left_median_volume_es_p5 entropy] 0.47\n",
      "[thr_left_std_volume_es_p5 entropy] 0.25\n",
      "[thr_right_mean_volume_es_p5 entropy] 0.32\n",
      "[thr_right_median_volume_es_p5 entropy] 0.47\n",
      "[thr_right_std_volume_es_p5 entropy] 0.25\n",
      "[u_turn_mean_volume_es_p5 entropy] 0.32\n",
      "[u_turn_median_volume_es_p5 entropy] 0.47\n",
      "[u_turn_std_volume_es_p5 entropy] 0.25\n",
      "[num_lanes_mean_volume_es_p5 entropy] 0.32\n",
      "[num_lanes_median_volume_es_p5 entropy] 0.47\n",
      "[num_lanes_std_volume_es_p5 entropy] 0.25\n",
      "[weekday_mean_volume_es_p5 entropy] 0.32\n",
      "[weekday_median_volume_es_p5 entropy] 0.46\n",
      "[weekday_std_volume_es_p5 entropy] 0.25\n",
      "[interval_mean_volume_es_p5 entropy] 1.40\n",
      "[interval_median_volume_es_p5 entropy] 1.92\n",
      "[interval_std_volume_es_p5 entropy] 0.55\n",
      "[holiday_mean_volume_es_p5 entropy] 0.32\n",
      "[holiday_median_volume_es_p5 entropy] 0.46\n",
      "[holiday_std_volume_es_p5 entropy] 0.25\n",
      "[peak_mean_volume_es_p5 entropy] 0.34\n",
      "[peak_median_volume_es_p5 entropy] 0.49\n",
      "[peak_std_volume_es_p5 entropy] 0.27\n",
      "[penetration entropy] 1.61\n",
      "[penetration_p6 entropy] 1.58\n",
      "[scaled_volume_0_0 entropy] inf\n",
      "[scaled_volume_0_1 entropy] inf\n",
      "[scaled_volume_0_2 entropy] inf\n",
      "[scaled_volume_1_0 entropy] inf\n",
      "[scaled_volume_1_1 entropy] inf\n",
      "[scaled_volume_1_2 entropy] inf\n",
      "[scaled_volume_2_0 entropy] inf\n",
      "[scaled_volume_2_1 entropy] inf\n",
      "[scaled_volume_2_2 entropy] inf\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = dataset_split(X, Y, 'random_transfer')\n",
    "\n",
    "for col in X_train.columns:\n",
    "    print(\"[{:s} entropy] {:.2f}\".format(col, entropy(X_train[col].iloc[:X_test.shape[-0]],\n",
    "                                                      X_test[col])))\n",
    "    #plt.figure()\n",
    "    #sns.distplot(X_train[col])\n",
    "    #sns.distplot(X_test[col])\n",
    "    #plt.savefig('../Data/feature_distribution_shift_figs/' + col + '_dist.png',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:21.199078Z",
     "start_time": "2020-03-24T05:14:21.195073Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_dense(h, units, drop=True, norm=True):\n",
    "    h = Dense(units=units,use_bias=True,activation=None,\n",
    "              kernel_initializer=he_normal(),\n",
    "              bias_initializer=constant(0.1),\n",
    "              kernel_regularizer=l2())(h)\n",
    "    if norm:\n",
    "        h = BatchNormalization()(h)\n",
    "    h = LeakyReLU(0.2)(h)\n",
    "    if drop:\n",
    "        h = Dropout(rate=0.1)(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:26:53.447003Z",
     "start_time": "2020-03-24T06:26:53.438028Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(input_shape, embedding_dim, feature_dim):\n",
    "    x_in = Input(shape=(input_shape,))\n",
    "    interval = Input(shape=(1,))\n",
    "    weekday = Input(shape=(1,))\n",
    "    holiday = Input(shape=(1,))\n",
    "    peak = Input(shape=(1,))\n",
    "    \n",
    "    v_itv = Flatten()(Embedding(input_dim=144,output_dim=int(2 * embedding_dim))(interval))\n",
    "    v_weekday = Flatten()(Embedding(input_dim=7,output_dim=embedding_dim)(weekday))\n",
    "    v_holiday = Flatten()(Embedding(input_dim=2,output_dim=embedding_dim)(holiday))\n",
    "    v_peak = Flatten()(Embedding(input_dim=4,output_dim=embedding_dim)(peak))\n",
    "    \n",
    "    vec = Concatenate()([x_in, v_itv, v_weekday, v_holiday, v_peak])\n",
    "    \n",
    "    h = build_dense(vec, 256, drop=False, norm=False)\n",
    "    h = build_dense(h, feature_dim, drop=False, norm=False)\n",
    "    \n",
    "    transformer = Model(inputs=[x_in,interval,weekday,\n",
    "                                holiday,peak],\n",
    "                        outputs=h,name='transformer')\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:27:01.191293Z",
     "start_time": "2020-03-24T06:27:01.184312Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_regressor(feature_dim):\n",
    "    x_in = Input(shape=(feature_dim,))\n",
    "    \n",
    "    def build_resblock(h):\n",
    "        h1 = build_dense(h, 128, drop=True, norm=False)\n",
    "        h2 = build_dense(h1, 256, drop=True, norm=False)\n",
    "        h3 = build_dense(h2, 128, drop=True, norm=False)\n",
    "        h4 = Add()([h1,h3])\n",
    "        return h4\n",
    "    \n",
    "    \n",
    "    h = build_resblock(x_in)\n",
    "    h = build_resblock(h)\n",
    "    h = build_resblock(h)\n",
    "    h = Dense(units=1,use_bias=True,\n",
    "              activation='relu',\n",
    "              kernel_initializer=he_normal(),\n",
    "              bias_initializer=constant(0.1),\n",
    "              kernel_regularizer=l2())(h)\n",
    "    \n",
    "    regressor = Model(inputs=x_in,outputs=h,name='regressor')\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:19:19.149238Z",
     "start_time": "2020-03-24T05:19:19.144251Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator(feature_dim):\n",
    "    x_in = Input(shape=(feature_dim,))\n",
    "    h = build_dense(x_in, 128, drop=False, norm=True)\n",
    "    h = build_dense(h, 64, drop=False, norm=True)\n",
    "    h = build_dense(h, 32, drop=False, norm=True)\n",
    "    h = Dense(units=1,use_bias=True,\n",
    "              activation='tanh',\n",
    "              kernel_initializer='uniform',\n",
    "              bias_initializer=constant(0.0),\n",
    "              kernel_regularizer=l2())(h)\n",
    "    \n",
    "    discriminator = Model(inputs=x_in,outputs=h,name='discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:19:26.945389Z",
     "start_time": "2020-03-24T05:19:26.938409Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_trd(transformer, discriminator, input_shape):\n",
    "    x_in = Input(shape=(input_shape,))\n",
    "    interval = Input(shape=(1,))\n",
    "    weekday = Input(shape=(1,))\n",
    "    holiday = Input(shape=(1,))\n",
    "    peak = Input(shape=(1,))\n",
    "      \n",
    "    invariant_features = transformer([x_in,interval,weekday,holiday,peak])\n",
    "    \n",
    "    class_pred = discriminator(invariant_features)\n",
    "    \n",
    "    trd = Model(inputs=[x_in,interval,weekday,holiday,peak],outputs=class_pred)\n",
    "    return trd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:27.698695Z",
     "start_time": "2020-03-24T05:14:27.692711Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_batch(X_source, Y_source, X_target, batch_size):\n",
    "    while True:\n",
    "        idx_source = np.random.choice(X_source.shape[0], batch_size, replace=False)\n",
    "        idx_target = np.random.choice(X_target.shape[0], batch_size, replace=False)\n",
    "        yield X_source[idx_source], Y_source[idx_source], X_target[idx_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:27.976948Z",
     "start_time": "2020-03-24T05:14:27.972958Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def wasserstein(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:28.384856Z",
     "start_time": "2020-03-24T05:14:28.381863Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == np.round(y_pred)) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:29.871005Z",
     "start_time": "2020-03-24T05:14:29.866999Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def disc_iteration(iter_idx, cycle_length, split, iter_l, iter_s):\n",
    "    c = iter_idx % cycle_length\n",
    "    if c < split:\n",
    "        return iter_l\n",
    "    else:\n",
    "        return iter_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:31:02.536200Z",
     "start_time": "2020-03-24T05:31:02.517251Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train_trd(X_source, Y_source, X_target, attr_cols,\n",
    "              embedding_dim, feature_dim,\n",
    "              lr_list, num_iterations,\n",
    "              cycle_length, split, iter_l, iter_s,\n",
    "              batch_size, display_num=50):\n",
    "    \n",
    "    # data init\n",
    "    cat_cols = attr_cols\n",
    "    num_cols = list(set(range(X_source.shape[1])) - set(cat_cols))\n",
    "    \n",
    "    # build models\n",
    "    assert X_source.shape[1] == X_target.shape[1]\n",
    "    input_shape = len(num_cols)\n",
    "    transformer = build_transformer(input_shape, embedding_dim, feature_dim)\n",
    "    regressor = build_regressor(feature_dim=feature_dim)\n",
    "    discriminator = build_discriminator(feature_dim=feature_dim)\n",
    "    trd = build_trd(transformer, discriminator, input_shape=input_shape)\n",
    "\n",
    "    # define optimizer & load lr\n",
    "    assert len(lr_list) == 3\n",
    "    opt_trans = Adam(lr_list[0])\n",
    "    opt_reg = Adam(lr_list[1])\n",
    "    opt_disc = Adam(lr_list[2])\n",
    "\n",
    "    # model compile\n",
    "    transformer.compile(loss='mse', optimizer=opt_trans)\n",
    "    print(\"[transformer params] {:d}\".format(transformer.count_params()))\n",
    "    \n",
    "    regressor.compile(loss='mse', optimizer=opt_reg)\n",
    "    print(\"[regressor params] {:d}\".format(regressor.count_params()))\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    trd.compile(loss='binary_crossentropy', optimizer=opt_trans)\n",
    "    print(\"[trd params] {:d}\".format(trd.count_params()))\n",
    "    discriminator.trainable = True\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=opt_disc)\n",
    "    print(\"[discriminator params] {:d}\\n\".format(discriminator.count_params()))\n",
    "\n",
    "    # ====================================================================\n",
    "    # training loop start\n",
    "    # ====================================================================\n",
    "\n",
    "    iter_idx = 0\n",
    "    num_iterations = num_iterations\n",
    "    log = []\n",
    "\n",
    "    while iter_idx < num_iterations:\n",
    "\n",
    "        # train discriminator\n",
    "        d_iter_idx = 0\n",
    "        if iter_idx < split or iter_idx % cycle_length == 0:\n",
    "            num_d_iterations = iter_l\n",
    "        else:\n",
    "            num_d_iterations = iter_s\n",
    "\n",
    "        while d_iter_idx < num_d_iterations:\n",
    "            batch_source_x, batch_source_y, batch_target_x = next(get_batch(\n",
    "                X_source, Y_source, X_target, batch_size\n",
    "            ))\n",
    "            class_source = np.zeros((batch_source_x.shape[0], 1))\n",
    "            class_target = np.ones((batch_source_x.shape[0], 1))\n",
    "\n",
    "            d_batch_x = np.concatenate((batch_source_x, batch_target_x))\n",
    "            d_batch_y = np.concatenate((class_source, class_target))\n",
    "            trans_output = transformer.predict([d_batch_x[:,num_cols],\n",
    "                                                d_batch_x[:,cat_cols[0]],\n",
    "                                                d_batch_x[:,cat_cols[1]],\n",
    "                                                d_batch_x[:,cat_cols[2]],\n",
    "                                                d_batch_x[:,cat_cols[3]]])\n",
    "            d_loss = discriminator.train_on_batch(trans_output, d_batch_y)\n",
    "            d_iter_idx += 1\n",
    "        \n",
    "        # re-batch\n",
    "        batch_source_x, batch_source_y, batch_target_x = next(get_batch(\n",
    "            X_source, Y_source, X_target, batch_size\n",
    "        ))\n",
    "        class_source = np.zeros((batch_source_x.shape[0], 1))\n",
    "        class_target = np.ones((batch_source_x.shape[0], 1))\n",
    "        \n",
    "        # train transformer\n",
    "        discriminator.trainable = False\n",
    "        t_loss = trd.train_on_batch([batch_target_x[:,num_cols],\n",
    "                                     batch_target_x[:,cat_cols[0]],\n",
    "                                     batch_target_x[:,cat_cols[1]],\n",
    "                                     batch_target_x[:,cat_cols[2]],\n",
    "                                     batch_target_x[:,cat_cols[3]]], class_source)\n",
    "        trans_output_reg = transformer.predict([batch_source_x[:,num_cols],\n",
    "                                                batch_source_x[:,cat_cols[0]],\n",
    "                                                batch_source_x[:,cat_cols[1]],\n",
    "                                                batch_source_x[:,cat_cols[2]],\n",
    "                                                batch_source_x[:,cat_cols[3]]])\n",
    "        \n",
    "        # disc acc.\n",
    "        if iter_idx % display_num == 0:\n",
    "            trans_output_disc = transformer.predict([batch_target_x[:,num_cols],\n",
    "                                                     batch_target_x[:,cat_cols[0]],\n",
    "                                                     batch_target_x[:,cat_cols[1]],\n",
    "                                                     batch_target_x[:,cat_cols[2]],\n",
    "                                                     batch_target_x[:,cat_cols[3]]])\n",
    "            negative_acc = accuracy(class_source, discriminator.predict(trans_output_reg))\n",
    "            positive_acc = accuracy(class_target, discriminator.predict(trans_output_disc))\n",
    "            print(\"[iteration] {:d} | [source acc.] {:.2f}% | [target acc.] {:.2f}%\".format(\n",
    "                    iter_idx, 100 * negative_acc, 100 * positive_acc\n",
    "                ))\n",
    "        \n",
    "        # train regressor\n",
    "        if iter_idx > 5000:\n",
    "            r_loss = regressor.train_on_batch(trans_output_reg, batch_source_y)\n",
    "        else:\n",
    "            r_loss = -1.0\n",
    "        \n",
    "        discriminator.trainable = True\n",
    "\n",
    "        log.append([d_loss, t_loss, r_loss])\n",
    "        if iter_idx % display_num == 0:\n",
    "            print(\"[iteration] {:d} | [d-loss] {:.2f} | [t-loss] {:.2f} | [r-loss] {:.2f}\".format(\n",
    "                iter_idx, d_loss, t_loss, r_loss\n",
    "            ))\n",
    "        iter_idx += 1\n",
    "\n",
    "    # ====================================================================\n",
    "    # training loop end\n",
    "    # ====================================================================\n",
    "    \n",
    "    return transformer, regressor, discriminator, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:45.035082Z",
     "start_time": "2020-03-24T05:14:45.029118Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(X, categorical_features):\n",
    "    one_hot_enc = OneHotEncoder()\n",
    "    for feat in categorical_features:\n",
    "        encoded_matrix = one_hot_enc.fit_transform(X[feat].values.reshape(-1,1)).toarray()\n",
    "        num_class = encoded_matrix.shape[1]\n",
    "        one_hot_features = pd.DataFrame()\n",
    "        for class_ in range(num_class):\n",
    "            one_hot_features[feat + '_' + str(class_)] = encoded_matrix[:,class_]\n",
    "        X = pd.concat((X, one_hot_features),axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:16:04.350232Z",
     "start_time": "2020-03-24T05:16:03.507489Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['weekday', 'holiday', 'peak', 'interval']\n",
    "numeric_features = list(set(X.columns) - set(categorical_features) - {'linkIdx','datetime'})\n",
    "X_nn = one_hot_encoding(X, categorical_features)\n",
    "\n",
    "split_mode = 'fix_transfer'\n",
    "X_train, X_test, y_train, y_test = dataset_split(X, Y, split_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:17:16.914800Z",
     "start_time": "2020-03-24T05:31:06.033848Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transformer params] 11745\n",
      "[regressor params] 263809\n",
      "[trd params] 55906\n",
      "[discriminator params] 44161\n",
      "\n",
      "[iteration] 0 | [source acc.] 96.88% | [target acc.] 25.00%\n",
      "[iteration] 0 | [d-loss] 2.44 | [t-loss] 7.58 | [r-loss] -1.00\n",
      "[iteration] 50 | [source acc.] 21.88% | [target acc.] 87.50%\n",
      "[iteration] 50 | [d-loss] 0.70 | [t-loss] 5.29 | [r-loss] -1.00\n",
      "[iteration] 100 | [source acc.] 68.75% | [target acc.] 65.62%\n",
      "[iteration] 100 | [d-loss] 0.66 | [t-loss] 4.83 | [r-loss] -1.00\n",
      "[iteration] 150 | [source acc.] 59.38% | [target acc.] 65.62%\n",
      "[iteration] 150 | [d-loss] 0.62 | [t-loss] 4.41 | [r-loss] -1.00\n",
      "[iteration] 200 | [source acc.] 53.12% | [target acc.] 68.75%\n",
      "[iteration] 200 | [d-loss] 0.65 | [t-loss] 4.03 | [r-loss] -1.00\n",
      "[iteration] 250 | [source acc.] 68.75% | [target acc.] 31.25%\n",
      "[iteration] 250 | [d-loss] 0.62 | [t-loss] 3.68 | [r-loss] -1.00\n",
      "[iteration] 300 | [source acc.] 78.12% | [target acc.] 50.00%\n",
      "[iteration] 300 | [d-loss] 0.66 | [t-loss] 3.42 | [r-loss] -1.00\n",
      "[iteration] 350 | [source acc.] 71.88% | [target acc.] 59.38%\n",
      "[iteration] 350 | [d-loss] 0.66 | [t-loss] 3.12 | [r-loss] -1.00\n",
      "[iteration] 400 | [source acc.] 53.12% | [target acc.] 59.38%\n",
      "[iteration] 400 | [d-loss] 0.60 | [t-loss] 2.87 | [r-loss] -1.00\n",
      "[iteration] 450 | [source acc.] 25.00% | [target acc.] 93.75%\n",
      "[iteration] 450 | [d-loss] 0.62 | [t-loss] 2.66 | [r-loss] -1.00\n",
      "[iteration] 500 | [source acc.] 81.25% | [target acc.] 56.25%\n",
      "[iteration] 500 | [d-loss] 0.67 | [t-loss] 2.45 | [r-loss] -1.00\n",
      "[iteration] 550 | [source acc.] 40.62% | [target acc.] 71.88%\n",
      "[iteration] 550 | [d-loss] 0.69 | [t-loss] 2.25 | [r-loss] -1.00\n",
      "[iteration] 600 | [source acc.] 65.62% | [target acc.] 84.38%\n",
      "[iteration] 600 | [d-loss] 0.63 | [t-loss] 2.11 | [r-loss] -1.00\n",
      "[iteration] 650 | [source acc.] 53.12% | [target acc.] 75.00%\n",
      "[iteration] 650 | [d-loss] 0.67 | [t-loss] 1.98 | [r-loss] -1.00\n",
      "[iteration] 700 | [source acc.] 75.00% | [target acc.] 59.38%\n",
      "[iteration] 700 | [d-loss] 0.64 | [t-loss] 1.87 | [r-loss] -1.00\n",
      "[iteration] 750 | [source acc.] 46.88% | [target acc.] 65.62%\n",
      "[iteration] 750 | [d-loss] 0.62 | [t-loss] 1.74 | [r-loss] -1.00\n",
      "[iteration] 800 | [source acc.] 50.00% | [target acc.] 65.62%\n",
      "[iteration] 800 | [d-loss] 0.62 | [t-loss] 1.60 | [r-loss] -1.00\n",
      "[iteration] 850 | [source acc.] 37.50% | [target acc.] 81.25%\n",
      "[iteration] 850 | [d-loss] 0.59 | [t-loss] 1.52 | [r-loss] -1.00\n",
      "[iteration] 900 | [source acc.] 34.38% | [target acc.] 62.50%\n",
      "[iteration] 900 | [d-loss] 0.62 | [t-loss] 1.43 | [r-loss] -1.00\n",
      "[iteration] 950 | [source acc.] 71.88% | [target acc.] 25.00%\n",
      "[iteration] 950 | [d-loss] 0.64 | [t-loss] 1.35 | [r-loss] -1.00\n",
      "[iteration] 1000 | [source acc.] 34.38% | [target acc.] 81.25%\n",
      "[iteration] 1000 | [d-loss] 0.64 | [t-loss] 1.29 | [r-loss] -1.00\n",
      "[iteration] 1050 | [source acc.] 56.25% | [target acc.] 81.25%\n",
      "[iteration] 1050 | [d-loss] 0.65 | [t-loss] 1.24 | [r-loss] -1.00\n",
      "[iteration] 1100 | [source acc.] 68.75% | [target acc.] 56.25%\n",
      "[iteration] 1100 | [d-loss] 0.64 | [t-loss] 1.18 | [r-loss] -1.00\n",
      "[iteration] 1150 | [source acc.] 59.38% | [target acc.] 71.88%\n",
      "[iteration] 1150 | [d-loss] 0.66 | [t-loss] 1.13 | [r-loss] -1.00\n",
      "[iteration] 1200 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 1200 | [d-loss] 0.64 | [t-loss] 1.08 | [r-loss] -1.00\n",
      "[iteration] 1250 | [source acc.] 53.12% | [target acc.] 50.00%\n",
      "[iteration] 1250 | [d-loss] 0.66 | [t-loss] 1.02 | [r-loss] -1.00\n",
      "[iteration] 1300 | [source acc.] 93.75% | [target acc.] 21.88%\n",
      "[iteration] 1300 | [d-loss] 0.63 | [t-loss] 1.00 | [r-loss] -1.00\n",
      "[iteration] 1350 | [source acc.] 93.75% | [target acc.] 18.75%\n",
      "[iteration] 1350 | [d-loss] 0.62 | [t-loss] 0.98 | [r-loss] -1.00\n",
      "[iteration] 1400 | [source acc.] 75.00% | [target acc.] 62.50%\n",
      "[iteration] 1400 | [d-loss] 0.66 | [t-loss] 0.95 | [r-loss] -1.00\n",
      "[iteration] 1450 | [source acc.] 59.38% | [target acc.] 62.50%\n",
      "[iteration] 1450 | [d-loss] 0.63 | [t-loss] 0.93 | [r-loss] -1.00\n",
      "[iteration] 1500 | [source acc.] 40.62% | [target acc.] 68.75%\n",
      "[iteration] 1500 | [d-loss] 0.61 | [t-loss] 0.91 | [r-loss] -1.00\n",
      "[iteration] 1550 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 1550 | [d-loss] 0.63 | [t-loss] 0.85 | [r-loss] -1.00\n",
      "[iteration] 1600 | [source acc.] 56.25% | [target acc.] 81.25%\n",
      "[iteration] 1600 | [d-loss] 0.63 | [t-loss] 0.84 | [r-loss] -1.00\n",
      "[iteration] 1650 | [source acc.] 53.12% | [target acc.] 78.12%\n",
      "[iteration] 1650 | [d-loss] 0.64 | [t-loss] 0.85 | [r-loss] -1.00\n",
      "[iteration] 1700 | [source acc.] 37.50% | [target acc.] 65.62%\n",
      "[iteration] 1700 | [d-loss] 0.73 | [t-loss] 0.87 | [r-loss] -1.00\n",
      "[iteration] 1750 | [source acc.] 75.00% | [target acc.] 50.00%\n",
      "[iteration] 1750 | [d-loss] 0.66 | [t-loss] 0.83 | [r-loss] -1.00\n",
      "[iteration] 1800 | [source acc.] 68.75% | [target acc.] 68.75%\n",
      "[iteration] 1800 | [d-loss] 0.65 | [t-loss] 0.81 | [r-loss] -1.00\n",
      "[iteration] 1850 | [source acc.] 21.88% | [target acc.] 84.38%\n",
      "[iteration] 1850 | [d-loss] 0.62 | [t-loss] 0.82 | [r-loss] -1.00\n",
      "[iteration] 1900 | [source acc.] 65.62% | [target acc.] 75.00%\n",
      "[iteration] 1900 | [d-loss] 0.62 | [t-loss] 0.81 | [r-loss] -1.00\n",
      "[iteration] 1950 | [source acc.] 50.00% | [target acc.] 59.38%\n",
      "[iteration] 1950 | [d-loss] 0.64 | [t-loss] 0.81 | [r-loss] -1.00\n",
      "[iteration] 2000 | [source acc.] 34.38% | [target acc.] 87.50%\n",
      "[iteration] 2000 | [d-loss] 0.64 | [t-loss] 0.81 | [r-loss] -1.00\n",
      "[iteration] 2050 | [source acc.] 15.62% | [target acc.] 90.62%\n",
      "[iteration] 2050 | [d-loss] 0.71 | [t-loss] 0.84 | [r-loss] -1.00\n",
      "[iteration] 2100 | [source acc.] 68.75% | [target acc.] 68.75%\n",
      "[iteration] 2100 | [d-loss] 0.66 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2150 | [source acc.] 59.38% | [target acc.] 59.38%\n",
      "[iteration] 2150 | [d-loss] 0.63 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2200 | [source acc.] 68.75% | [target acc.] 56.25%\n",
      "[iteration] 2200 | [d-loss] 0.63 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2250 | [source acc.] 87.50% | [target acc.] 50.00%\n",
      "[iteration] 2250 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 2300 | [source acc.] 43.75% | [target acc.] 90.62%\n",
      "[iteration] 2300 | [d-loss] 0.67 | [t-loss] 0.82 | [r-loss] -1.00\n",
      "[iteration] 2350 | [source acc.] 62.50% | [target acc.] 62.50%\n",
      "[iteration] 2350 | [d-loss] 0.67 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2400 | [source acc.] 68.75% | [target acc.] 56.25%\n",
      "[iteration] 2400 | [d-loss] 0.61 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 2450 | [source acc.] 87.50% | [target acc.] 31.25%\n",
      "[iteration] 2450 | [d-loss] 0.61 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2500 | [source acc.] 50.00% | [target acc.] 75.00%\n",
      "[iteration] 2500 | [d-loss] 0.63 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 2550 | [source acc.] 65.62% | [target acc.] 68.75%\n",
      "[iteration] 2550 | [d-loss] 0.63 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 2600 | [source acc.] 43.75% | [target acc.] 90.62%\n",
      "[iteration] 2600 | [d-loss] 0.69 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2650 | [source acc.] 78.12% | [target acc.] 21.88%\n",
      "[iteration] 2650 | [d-loss] 0.60 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 2700 | [source acc.] 53.12% | [target acc.] 34.38%\n",
      "[iteration] 2700 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 2750 | [source acc.] 71.88% | [target acc.] 50.00%\n",
      "[iteration] 2750 | [d-loss] 0.74 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 2800 | [source acc.] 65.62% | [target acc.] 62.50%\n",
      "[iteration] 2800 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 2850 | [source acc.] 46.88% | [target acc.] 87.50%\n",
      "[iteration] 2850 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 2900 | [source acc.] 40.62% | [target acc.] 59.38%\n",
      "[iteration] 2900 | [d-loss] 0.62 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 2950 | [source acc.] 40.62% | [target acc.] 68.75%\n",
      "[iteration] 2950 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3000 | [source acc.] 53.12% | [target acc.] 59.38%\n",
      "[iteration] 3000 | [d-loss] 0.59 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 3050 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 3050 | [d-loss] 0.75 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3100 | [source acc.] 59.38% | [target acc.] 50.00%\n",
      "[iteration] 3100 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3150 | [source acc.] 62.50% | [target acc.] 84.38%\n",
      "[iteration] 3150 | [d-loss] 0.71 | [t-loss] 0.75 | [r-loss] -1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration] 3200 | [source acc.] 78.12% | [target acc.] 59.38%\n",
      "[iteration] 3200 | [d-loss] 0.69 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 3250 | [source acc.] 62.50% | [target acc.] 40.62%\n",
      "[iteration] 3250 | [d-loss] 0.65 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 3300 | [source acc.] 34.38% | [target acc.] 75.00%\n",
      "[iteration] 3300 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 3350 | [source acc.] 71.88% | [target acc.] 21.88%\n",
      "[iteration] 3350 | [d-loss] 0.59 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 3400 | [source acc.] 68.75% | [target acc.] 46.88%\n",
      "[iteration] 3400 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 3450 | [source acc.] 75.00% | [target acc.] 50.00%\n",
      "[iteration] 3450 | [d-loss] 0.62 | [t-loss] 0.78 | [r-loss] -1.00\n",
      "[iteration] 3500 | [source acc.] 59.38% | [target acc.] 65.62%\n",
      "[iteration] 3500 | [d-loss] 0.74 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 3550 | [source acc.] 53.12% | [target acc.] 50.00%\n",
      "[iteration] 3550 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 3600 | [source acc.] 53.12% | [target acc.] 62.50%\n",
      "[iteration] 3600 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3650 | [source acc.] 50.00% | [target acc.] 56.25%\n",
      "[iteration] 3650 | [d-loss] 0.63 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 3700 | [source acc.] 90.62% | [target acc.] 21.88%\n",
      "[iteration] 3700 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3750 | [source acc.] 59.38% | [target acc.] 62.50%\n",
      "[iteration] 3750 | [d-loss] 0.72 | [t-loss] 0.73 | [r-loss] -1.00\n",
      "[iteration] 3800 | [source acc.] 59.38% | [target acc.] 40.62%\n",
      "[iteration] 3800 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3850 | [source acc.] 53.12% | [target acc.] 71.88%\n",
      "[iteration] 3850 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 3900 | [source acc.] 56.25% | [target acc.] 75.00%\n",
      "[iteration] 3900 | [d-loss] 0.62 | [t-loss] 0.73 | [r-loss] -1.00\n",
      "[iteration] 3950 | [source acc.] 68.75% | [target acc.] 28.12%\n",
      "[iteration] 3950 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 4000 | [source acc.] 43.75% | [target acc.] 68.75%\n",
      "[iteration] 4000 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 4050 | [source acc.] 37.50% | [target acc.] 87.50%\n",
      "[iteration] 4050 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 4100 | [source acc.] 56.25% | [target acc.] 68.75%\n",
      "[iteration] 4100 | [d-loss] 0.68 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 4150 | [source acc.] 81.25% | [target acc.] 3.12%\n",
      "[iteration] 4150 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 4200 | [source acc.] 68.75% | [target acc.] 43.75%\n",
      "[iteration] 4200 | [d-loss] 0.75 | [t-loss] 0.73 | [r-loss] -1.00\n",
      "[iteration] 4250 | [source acc.] 43.75% | [target acc.] 71.88%\n",
      "[iteration] 4250 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 4300 | [source acc.] 34.38% | [target acc.] 75.00%\n",
      "[iteration] 4300 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 4350 | [source acc.] 37.50% | [target acc.] 59.38%\n",
      "[iteration] 4350 | [d-loss] 0.61 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 4400 | [source acc.] 71.88% | [target acc.] 56.25%\n",
      "[iteration] 4400 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 4450 | [source acc.] 65.62% | [target acc.] 56.25%\n",
      "[iteration] 4450 | [d-loss] 0.67 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 4500 | [source acc.] 21.88% | [target acc.] 71.88%\n",
      "[iteration] 4500 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 4550 | [source acc.] 50.00% | [target acc.] 62.50%\n",
      "[iteration] 4550 | [d-loss] 0.69 | [t-loss] 0.73 | [r-loss] -1.00\n",
      "[iteration] 4600 | [source acc.] 68.75% | [target acc.] 56.25%\n",
      "[iteration] 4600 | [d-loss] 0.63 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 4650 | [source acc.] 40.62% | [target acc.] 78.12%\n",
      "[iteration] 4650 | [d-loss] 0.58 | [t-loss] 0.73 | [r-loss] -1.00\n",
      "[iteration] 4700 | [source acc.] 71.88% | [target acc.] 31.25%\n",
      "[iteration] 4700 | [d-loss] 0.62 | [t-loss] 0.76 | [r-loss] -1.00\n",
      "[iteration] 4750 | [source acc.] 62.50% | [target acc.] 59.38%\n",
      "[iteration] 4750 | [d-loss] 0.65 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 4800 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 4800 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 4850 | [source acc.] 46.88% | [target acc.] 78.12%\n",
      "[iteration] 4850 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] -1.00\n",
      "[iteration] 4900 | [source acc.] 40.62% | [target acc.] 46.88%\n",
      "[iteration] 4900 | [d-loss] 0.68 | [t-loss] 0.74 | [r-loss] -1.00\n",
      "[iteration] 4950 | [source acc.] 15.62% | [target acc.] 93.75%\n",
      "[iteration] 4950 | [d-loss] 0.75 | [t-loss] 0.79 | [r-loss] -1.00\n",
      "[iteration] 5000 | [source acc.] 25.00% | [target acc.] 87.50%\n",
      "[iteration] 5000 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] -1.00\n",
      "[iteration] 5050 | [source acc.] 37.50% | [target acc.] 50.00%\n",
      "[iteration] 5050 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 7081.16\n",
      "[iteration] 5100 | [source acc.] 71.88% | [target acc.] 59.38%\n",
      "[iteration] 5100 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 7893.95\n",
      "[iteration] 5150 | [source acc.] 34.38% | [target acc.] 81.25%\n",
      "[iteration] 5150 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 5261.44\n",
      "[iteration] 5200 | [source acc.] 50.00% | [target acc.] 84.38%\n",
      "[iteration] 5200 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 4524.63\n",
      "[iteration] 5250 | [source acc.] 43.75% | [target acc.] 87.50%\n",
      "[iteration] 5250 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 7856.40\n",
      "[iteration] 5300 | [source acc.] 81.25% | [target acc.] 50.00%\n",
      "[iteration] 5300 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 6595.77\n",
      "[iteration] 5350 | [source acc.] 31.25% | [target acc.] 78.12%\n",
      "[iteration] 5350 | [d-loss] 0.62 | [t-loss] 0.74 | [r-loss] 5417.94\n",
      "[iteration] 5400 | [source acc.] 21.88% | [target acc.] 87.50%\n",
      "[iteration] 5400 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 6195.56\n",
      "[iteration] 5450 | [source acc.] 62.50% | [target acc.] 43.75%\n",
      "[iteration] 5450 | [d-loss] 0.69 | [t-loss] 0.77 | [r-loss] 4283.02\n",
      "[iteration] 5500 | [source acc.] 78.12% | [target acc.] 50.00%\n",
      "[iteration] 5500 | [d-loss] 0.58 | [t-loss] 0.76 | [r-loss] 6337.73\n",
      "[iteration] 5550 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 5550 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4221.58\n",
      "[iteration] 5600 | [source acc.] 62.50% | [target acc.] 43.75%\n",
      "[iteration] 5600 | [d-loss] 0.67 | [t-loss] 0.77 | [r-loss] 6718.25\n",
      "[iteration] 5650 | [source acc.] 40.62% | [target acc.] 71.88%\n",
      "[iteration] 5650 | [d-loss] 0.60 | [t-loss] 0.74 | [r-loss] 3118.02\n",
      "[iteration] 5700 | [source acc.] 78.12% | [target acc.] 31.25%\n",
      "[iteration] 5700 | [d-loss] 0.70 | [t-loss] 0.77 | [r-loss] 8078.37\n",
      "[iteration] 5750 | [source acc.] 81.25% | [target acc.] 37.50%\n",
      "[iteration] 5750 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 5399.48\n",
      "[iteration] 5800 | [source acc.] 71.88% | [target acc.] 62.50%\n",
      "[iteration] 5800 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 4782.56\n",
      "[iteration] 5850 | [source acc.] 78.12% | [target acc.] 40.62%\n",
      "[iteration] 5850 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 4912.69\n",
      "[iteration] 5900 | [source acc.] 84.38% | [target acc.] 40.62%\n",
      "[iteration] 5900 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 3606.62\n",
      "[iteration] 5950 | [source acc.] 59.38% | [target acc.] 84.38%\n",
      "[iteration] 5950 | [d-loss] 0.61 | [t-loss] 0.73 | [r-loss] 7712.11\n",
      "[iteration] 6000 | [source acc.] 50.00% | [target acc.] 65.62%\n",
      "[iteration] 6000 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] 5008.63\n",
      "[iteration] 6050 | [source acc.] 53.12% | [target acc.] 56.25%\n",
      "[iteration] 6050 | [d-loss] 0.66 | [t-loss] 0.78 | [r-loss] 5535.40\n",
      "[iteration] 6100 | [source acc.] 87.50% | [target acc.] 40.62%\n",
      "[iteration] 6100 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 3964.47\n",
      "[iteration] 6150 | [source acc.] 81.25% | [target acc.] 37.50%\n",
      "[iteration] 6150 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 5703.68\n",
      "[iteration] 6200 | [source acc.] 68.75% | [target acc.] 50.00%\n",
      "[iteration] 6200 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5042.99\n",
      "[iteration] 6250 | [source acc.] 37.50% | [target acc.] 71.88%\n",
      "[iteration] 6250 | [d-loss] 0.80 | [t-loss] 0.75 | [r-loss] 4335.90\n",
      "[iteration] 6300 | [source acc.] 68.75% | [target acc.] 56.25%\n",
      "[iteration] 6300 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 5654.41\n",
      "[iteration] 6350 | [source acc.] 56.25% | [target acc.] 59.38%\n",
      "[iteration] 6350 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 6756.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration] 6400 | [source acc.] 87.50% | [target acc.] 34.38%\n",
      "[iteration] 6400 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] 8692.28\n",
      "[iteration] 6450 | [source acc.] 43.75% | [target acc.] 71.88%\n",
      "[iteration] 6450 | [d-loss] 0.72 | [t-loss] 0.75 | [r-loss] 4380.21\n",
      "[iteration] 6500 | [source acc.] 46.88% | [target acc.] 75.00%\n",
      "[iteration] 6500 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 5041.28\n",
      "[iteration] 6550 | [source acc.] 68.75% | [target acc.] 37.50%\n",
      "[iteration] 6550 | [d-loss] 0.63 | [t-loss] 0.76 | [r-loss] 5488.75\n",
      "[iteration] 6600 | [source acc.] 75.00% | [target acc.] 59.38%\n",
      "[iteration] 6600 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4910.80\n",
      "[iteration] 6650 | [source acc.] 46.88% | [target acc.] 81.25%\n",
      "[iteration] 6650 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 7111.66\n",
      "[iteration] 6700 | [source acc.] 75.00% | [target acc.] 40.62%\n",
      "[iteration] 6700 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 7209.75\n",
      "[iteration] 6750 | [source acc.] 65.62% | [target acc.] 46.88%\n",
      "[iteration] 6750 | [d-loss] 0.72 | [t-loss] 0.74 | [r-loss] 5541.28\n",
      "[iteration] 6800 | [source acc.] 37.50% | [target acc.] 71.88%\n",
      "[iteration] 6800 | [d-loss] 0.60 | [t-loss] 0.75 | [r-loss] 7577.81\n",
      "[iteration] 6850 | [source acc.] 37.50% | [target acc.] 78.12%\n",
      "[iteration] 6850 | [d-loss] 0.62 | [t-loss] 0.74 | [r-loss] 4845.37\n",
      "[iteration] 6900 | [source acc.] 68.75% | [target acc.] 68.75%\n",
      "[iteration] 6900 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 4507.41\n",
      "[iteration] 6950 | [source acc.] 62.50% | [target acc.] 81.25%\n",
      "[iteration] 6950 | [d-loss] 0.60 | [t-loss] 0.78 | [r-loss] 4453.83\n",
      "[iteration] 7000 | [source acc.] 59.38% | [target acc.] 53.12%\n",
      "[iteration] 7000 | [d-loss] 0.69 | [t-loss] 0.75 | [r-loss] 5087.46\n",
      "[iteration] 7050 | [source acc.] 68.75% | [target acc.] 59.38%\n",
      "[iteration] 7050 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 5916.08\n",
      "[iteration] 7100 | [source acc.] 75.00% | [target acc.] 59.38%\n",
      "[iteration] 7100 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5396.44\n",
      "[iteration] 7150 | [source acc.] 81.25% | [target acc.] 43.75%\n",
      "[iteration] 7150 | [d-loss] 0.68 | [t-loss] 0.74 | [r-loss] 3875.20\n",
      "[iteration] 7200 | [source acc.] 78.12% | [target acc.] 53.12%\n",
      "[iteration] 7200 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5412.89\n",
      "[iteration] 7250 | [source acc.] 84.38% | [target acc.] 56.25%\n",
      "[iteration] 7250 | [d-loss] 0.68 | [t-loss] 0.73 | [r-loss] 5080.76\n",
      "[iteration] 7300 | [source acc.] 43.75% | [target acc.] 71.88%\n",
      "[iteration] 7300 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 6813.90\n",
      "[iteration] 7350 | [source acc.] 84.38% | [target acc.] 46.88%\n",
      "[iteration] 7350 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] 5820.67\n",
      "[iteration] 7400 | [source acc.] 81.25% | [target acc.] 50.00%\n",
      "[iteration] 7400 | [d-loss] 0.68 | [t-loss] 0.74 | [r-loss] 9089.81\n",
      "[iteration] 7450 | [source acc.] 68.75% | [target acc.] 59.38%\n",
      "[iteration] 7450 | [d-loss] 0.70 | [t-loss] 0.73 | [r-loss] 6274.10\n",
      "[iteration] 7500 | [source acc.] 65.62% | [target acc.] 46.88%\n",
      "[iteration] 7500 | [d-loss] 0.67 | [t-loss] 0.76 | [r-loss] 4297.69\n",
      "[iteration] 7550 | [source acc.] 68.75% | [target acc.] 65.62%\n",
      "[iteration] 7550 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 4531.97\n",
      "[iteration] 7600 | [source acc.] 65.62% | [target acc.] 53.12%\n",
      "[iteration] 7600 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 5365.25\n",
      "[iteration] 7650 | [source acc.] 43.75% | [target acc.] 62.50%\n",
      "[iteration] 7650 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 4411.88\n",
      "[iteration] 7700 | [source acc.] 50.00% | [target acc.] 68.75%\n",
      "[iteration] 7700 | [d-loss] 0.61 | [t-loss] 0.74 | [r-loss] 4907.33\n",
      "[iteration] 7750 | [source acc.] 71.88% | [target acc.] 71.88%\n",
      "[iteration] 7750 | [d-loss] 0.64 | [t-loss] 0.73 | [r-loss] 5288.97\n",
      "[iteration] 7800 | [source acc.] 37.50% | [target acc.] 87.50%\n",
      "[iteration] 7800 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 4773.01\n",
      "[iteration] 7850 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 7850 | [d-loss] 0.61 | [t-loss] 0.75 | [r-loss] 6103.66\n",
      "[iteration] 7900 | [source acc.] 65.62% | [target acc.] 71.88%\n",
      "[iteration] 7900 | [d-loss] 0.60 | [t-loss] 0.74 | [r-loss] 7535.38\n",
      "[iteration] 7950 | [source acc.] 40.62% | [target acc.] 84.38%\n",
      "[iteration] 7950 | [d-loss] 0.61 | [t-loss] 0.73 | [r-loss] 5985.97\n",
      "[iteration] 8000 | [source acc.] 62.50% | [target acc.] 28.12%\n",
      "[iteration] 8000 | [d-loss] 0.72 | [t-loss] 0.75 | [r-loss] 4911.54\n",
      "[iteration] 8050 | [source acc.] 50.00% | [target acc.] 59.38%\n",
      "[iteration] 8050 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 5779.74\n",
      "[iteration] 8100 | [source acc.] 84.38% | [target acc.] 43.75%\n",
      "[iteration] 8100 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 6546.55\n",
      "[iteration] 8150 | [source acc.] 43.75% | [target acc.] 81.25%\n",
      "[iteration] 8150 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 6870.27\n",
      "[iteration] 8200 | [source acc.] 81.25% | [target acc.] 43.75%\n",
      "[iteration] 8200 | [d-loss] 0.74 | [t-loss] 0.75 | [r-loss] 5607.87\n",
      "[iteration] 8250 | [source acc.] 78.12% | [target acc.] 21.88%\n",
      "[iteration] 8250 | [d-loss] 0.69 | [t-loss] 0.74 | [r-loss] 5017.19\n",
      "[iteration] 8300 | [source acc.] 34.38% | [target acc.] 65.62%\n",
      "[iteration] 8300 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] 6584.14\n",
      "[iteration] 8350 | [source acc.] 71.88% | [target acc.] 62.50%\n",
      "[iteration] 8350 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 4439.72\n",
      "[iteration] 8400 | [source acc.] 81.25% | [target acc.] 34.38%\n",
      "[iteration] 8400 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] 5425.57\n",
      "[iteration] 8450 | [source acc.] 40.62% | [target acc.] 65.62%\n",
      "[iteration] 8450 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 4053.08\n",
      "[iteration] 8500 | [source acc.] 40.62% | [target acc.] 68.75%\n",
      "[iteration] 8500 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 7107.14\n",
      "[iteration] 8550 | [source acc.] 68.75% | [target acc.] 34.38%\n",
      "[iteration] 8550 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 5604.94\n",
      "[iteration] 8600 | [source acc.] 6.25% | [target acc.] 100.00%\n",
      "[iteration] 8600 | [d-loss] 0.71 | [t-loss] 0.74 | [r-loss] 5040.62\n",
      "[iteration] 8650 | [source acc.] 100.00% | [target acc.] 0.00%\n",
      "[iteration] 8650 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 6182.46\n",
      "[iteration] 8700 | [source acc.] 46.88% | [target acc.] 50.00%\n",
      "[iteration] 8700 | [d-loss] 0.71 | [t-loss] 0.74 | [r-loss] 6686.77\n",
      "[iteration] 8750 | [source acc.] 46.88% | [target acc.] 62.50%\n",
      "[iteration] 8750 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 6914.69\n",
      "[iteration] 8800 | [source acc.] 68.75% | [target acc.] 46.88%\n",
      "[iteration] 8800 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 5964.20\n",
      "[iteration] 8850 | [source acc.] 59.38% | [target acc.] 65.62%\n",
      "[iteration] 8850 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 5156.02\n",
      "[iteration] 8900 | [source acc.] 81.25% | [target acc.] 53.12%\n",
      "[iteration] 8900 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 4717.93\n",
      "[iteration] 8950 | [source acc.] 87.50% | [target acc.] 43.75%\n",
      "[iteration] 8950 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 5938.37\n",
      "[iteration] 9000 | [source acc.] 46.88% | [target acc.] 62.50%\n",
      "[iteration] 9000 | [d-loss] 0.69 | [t-loss] 0.73 | [r-loss] 6043.44\n",
      "[iteration] 9050 | [source acc.] 53.12% | [target acc.] 81.25%\n",
      "[iteration] 9050 | [d-loss] 0.66 | [t-loss] 0.73 | [r-loss] 5141.39\n",
      "[iteration] 9100 | [source acc.] 34.38% | [target acc.] 71.88%\n",
      "[iteration] 9100 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 5330.99\n",
      "[iteration] 9150 | [source acc.] 90.62% | [target acc.] 43.75%\n",
      "[iteration] 9150 | [d-loss] 0.71 | [t-loss] 0.73 | [r-loss] 3320.67\n",
      "[iteration] 9200 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 9200 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 7139.87\n",
      "[iteration] 9250 | [source acc.] 71.88% | [target acc.] 46.88%\n",
      "[iteration] 9250 | [d-loss] 0.68 | [t-loss] 0.74 | [r-loss] 5278.89\n",
      "[iteration] 9300 | [source acc.] 100.00% | [target acc.] 0.00%\n",
      "[iteration] 9300 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 4169.25\n",
      "[iteration] 9350 | [source acc.] 78.12% | [target acc.] 37.50%\n",
      "[iteration] 9350 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 3875.56\n",
      "[iteration] 9400 | [source acc.] 46.88% | [target acc.] 84.38%\n",
      "[iteration] 9400 | [d-loss] 0.71 | [t-loss] 0.73 | [r-loss] 5053.88\n",
      "[iteration] 9450 | [source acc.] 87.50% | [target acc.] 34.38%\n",
      "[iteration] 9450 | [d-loss] 0.70 | [t-loss] 0.73 | [r-loss] 4408.04\n",
      "[iteration] 9500 | [source acc.] 37.50% | [target acc.] 75.00%\n",
      "[iteration] 9500 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 5931.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration] 9550 | [source acc.] 37.50% | [target acc.] 87.50%\n",
      "[iteration] 9550 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 6777.05\n",
      "[iteration] 9600 | [source acc.] 50.00% | [target acc.] 59.38%\n",
      "[iteration] 9600 | [d-loss] 0.66 | [t-loss] 0.73 | [r-loss] 4486.93\n",
      "[iteration] 9650 | [source acc.] 75.00% | [target acc.] 53.12%\n",
      "[iteration] 9650 | [d-loss] 0.73 | [t-loss] 0.73 | [r-loss] 4449.49\n",
      "[iteration] 9700 | [source acc.] 68.75% | [target acc.] 46.88%\n",
      "[iteration] 9700 | [d-loss] 0.69 | [t-loss] 0.73 | [r-loss] 4809.68\n",
      "[iteration] 9750 | [source acc.] 78.12% | [target acc.] 34.38%\n",
      "[iteration] 9750 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 6471.61\n",
      "[iteration] 9800 | [source acc.] 75.00% | [target acc.] 37.50%\n",
      "[iteration] 9800 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 4625.67\n",
      "[iteration] 9850 | [source acc.] 43.75% | [target acc.] 81.25%\n",
      "[iteration] 9850 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 5125.86\n",
      "[iteration] 9900 | [source acc.] 96.88% | [target acc.] 21.88%\n",
      "[iteration] 9900 | [d-loss] 0.68 | [t-loss] 0.72 | [r-loss] 6212.25\n",
      "[iteration] 9950 | [source acc.] 71.88% | [target acc.] 62.50%\n",
      "[iteration] 9950 | [d-loss] 0.66 | [t-loss] 0.73 | [r-loss] 4000.48\n",
      "[iteration] 10000 | [source acc.] 46.88% | [target acc.] 75.00%\n",
      "[iteration] 10000 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4381.30\n",
      "[iteration] 10050 | [source acc.] 21.88% | [target acc.] 96.88%\n",
      "[iteration] 10050 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 4045.85\n",
      "[iteration] 10100 | [source acc.] 78.12% | [target acc.] 40.62%\n",
      "[iteration] 10100 | [d-loss] 0.63 | [t-loss] 0.72 | [r-loss] 3567.48\n",
      "[iteration] 10150 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 10150 | [d-loss] 0.68 | [t-loss] 0.70 | [r-loss] 4290.96\n",
      "[iteration] 10200 | [source acc.] 31.25% | [target acc.] 87.50%\n",
      "[iteration] 10200 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 4238.80\n",
      "[iteration] 10250 | [source acc.] 31.25% | [target acc.] 68.75%\n",
      "[iteration] 10250 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 5430.76\n",
      "[iteration] 10300 | [source acc.] 81.25% | [target acc.] 50.00%\n",
      "[iteration] 10300 | [d-loss] 0.70 | [t-loss] 0.74 | [r-loss] 8354.66\n",
      "[iteration] 10350 | [source acc.] 62.50% | [target acc.] 68.75%\n",
      "[iteration] 10350 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 5915.16\n",
      "[iteration] 10400 | [source acc.] 100.00% | [target acc.] 0.00%\n",
      "[iteration] 10400 | [d-loss] 0.69 | [t-loss] 0.73 | [r-loss] 4677.75\n",
      "[iteration] 10450 | [source acc.] 71.88% | [target acc.] 28.12%\n",
      "[iteration] 10450 | [d-loss] 0.66 | [t-loss] 0.73 | [r-loss] 5180.47\n",
      "[iteration] 10500 | [source acc.] 59.38% | [target acc.] 65.62%\n",
      "[iteration] 10500 | [d-loss] 0.72 | [t-loss] 0.77 | [r-loss] 5269.58\n",
      "[iteration] 10550 | [source acc.] 75.00% | [target acc.] 56.25%\n",
      "[iteration] 10550 | [d-loss] 0.65 | [t-loss] 0.73 | [r-loss] 4788.31\n",
      "[iteration] 10600 | [source acc.] 25.00% | [target acc.] 78.12%\n",
      "[iteration] 10600 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 4341.30\n",
      "[iteration] 10650 | [source acc.] 96.88% | [target acc.] 43.75%\n",
      "[iteration] 10650 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 5587.22\n",
      "[iteration] 10700 | [source acc.] 21.88% | [target acc.] 87.50%\n",
      "[iteration] 10700 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 6953.55\n",
      "[iteration] 10750 | [source acc.] 9.38% | [target acc.] 93.75%\n",
      "[iteration] 10750 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 7438.28\n",
      "[iteration] 10800 | [source acc.] 75.00% | [target acc.] 62.50%\n",
      "[iteration] 10800 | [d-loss] 0.62 | [t-loss] 0.73 | [r-loss] 5732.26\n",
      "[iteration] 10850 | [source acc.] 87.50% | [target acc.] 25.00%\n",
      "[iteration] 10850 | [d-loss] 0.67 | [t-loss] 0.72 | [r-loss] 4215.61\n",
      "[iteration] 10900 | [source acc.] 71.88% | [target acc.] 62.50%\n",
      "[iteration] 10900 | [d-loss] 0.70 | [t-loss] 0.74 | [r-loss] 6310.74\n",
      "[iteration] 10950 | [source acc.] 100.00% | [target acc.] 15.62%\n",
      "[iteration] 10950 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] 7145.89\n",
      "[iteration] 11000 | [source acc.] 71.88% | [target acc.] 40.62%\n",
      "[iteration] 11000 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 3022.11\n",
      "[iteration] 11050 | [source acc.] 56.25% | [target acc.] 56.25%\n",
      "[iteration] 11050 | [d-loss] 0.71 | [t-loss] 0.73 | [r-loss] 7273.02\n",
      "[iteration] 11100 | [source acc.] 75.00% | [target acc.] 28.12%\n",
      "[iteration] 11100 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 5230.61\n",
      "[iteration] 11150 | [source acc.] 68.75% | [target acc.] 62.50%\n",
      "[iteration] 11150 | [d-loss] 0.72 | [t-loss] 0.75 | [r-loss] 4866.71\n",
      "[iteration] 11200 | [source acc.] 90.62% | [target acc.] 31.25%\n",
      "[iteration] 11200 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 4995.17\n",
      "[iteration] 11250 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 11250 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4244.99\n",
      "[iteration] 11300 | [source acc.] 78.12% | [target acc.] 40.62%\n",
      "[iteration] 11300 | [d-loss] 0.66 | [t-loss] 0.73 | [r-loss] 5523.09\n",
      "[iteration] 11350 | [source acc.] 62.50% | [target acc.] 40.62%\n",
      "[iteration] 11350 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 4728.55\n",
      "[iteration] 11400 | [source acc.] 68.75% | [target acc.] 46.88%\n",
      "[iteration] 11400 | [d-loss] 0.71 | [t-loss] 0.73 | [r-loss] 4855.29\n",
      "[iteration] 11450 | [source acc.] 68.75% | [target acc.] 43.75%\n",
      "[iteration] 11450 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 5755.73\n",
      "[iteration] 11500 | [source acc.] 37.50% | [target acc.] 81.25%\n",
      "[iteration] 11500 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 4792.48\n",
      "[iteration] 11550 | [source acc.] 0.00% | [target acc.] 100.00%\n",
      "[iteration] 11550 | [d-loss] 0.69 | [t-loss] 0.74 | [r-loss] 5462.25\n",
      "[iteration] 11600 | [source acc.] 59.38% | [target acc.] 78.12%\n",
      "[iteration] 11600 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 6189.54\n",
      "[iteration] 11650 | [source acc.] 65.62% | [target acc.] 40.62%\n",
      "[iteration] 11650 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 6713.44\n",
      "[iteration] 11700 | [source acc.] 59.38% | [target acc.] 50.00%\n",
      "[iteration] 11700 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 4433.25\n",
      "[iteration] 11750 | [source acc.] 56.25% | [target acc.] 59.38%\n",
      "[iteration] 11750 | [d-loss] 0.70 | [t-loss] 0.74 | [r-loss] 5009.45\n",
      "[iteration] 11800 | [source acc.] 78.12% | [target acc.] 71.88%\n",
      "[iteration] 11800 | [d-loss] 0.68 | [t-loss] 0.74 | [r-loss] 4643.17\n",
      "[iteration] 11850 | [source acc.] 84.38% | [target acc.] 40.62%\n",
      "[iteration] 11850 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 5092.92\n",
      "[iteration] 11900 | [source acc.] 50.00% | [target acc.] 65.62%\n",
      "[iteration] 11900 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4513.30\n",
      "[iteration] 11950 | [source acc.] 68.75% | [target acc.] 50.00%\n",
      "[iteration] 11950 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 4883.85\n",
      "[iteration] 12000 | [source acc.] 62.50% | [target acc.] 50.00%\n",
      "[iteration] 12000 | [d-loss] 0.70 | [t-loss] 0.73 | [r-loss] 3988.63\n",
      "[iteration] 12050 | [source acc.] 62.50% | [target acc.] 46.88%\n",
      "[iteration] 12050 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 5229.17\n",
      "[iteration] 12100 | [source acc.] 56.25% | [target acc.] 53.12%\n",
      "[iteration] 12100 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 3794.47\n",
      "[iteration] 12150 | [source acc.] 75.00% | [target acc.] 34.38%\n",
      "[iteration] 12150 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 5312.36\n",
      "[iteration] 12200 | [source acc.] 34.38% | [target acc.] 75.00%\n",
      "[iteration] 12200 | [d-loss] 0.61 | [t-loss] 0.74 | [r-loss] 5118.12\n",
      "[iteration] 12250 | [source acc.] 87.50% | [target acc.] 50.00%\n",
      "[iteration] 12250 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 7567.01\n",
      "[iteration] 12300 | [source acc.] 75.00% | [target acc.] 56.25%\n",
      "[iteration] 12300 | [d-loss] 0.76 | [t-loss] 0.73 | [r-loss] 6003.48\n",
      "[iteration] 12350 | [source acc.] 59.38% | [target acc.] 53.12%\n",
      "[iteration] 12350 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 5844.80\n",
      "[iteration] 12400 | [source acc.] 68.75% | [target acc.] 50.00%\n",
      "[iteration] 12400 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 6659.47\n",
      "[iteration] 12450 | [source acc.] 81.25% | [target acc.] 28.12%\n",
      "[iteration] 12450 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 6043.85\n",
      "[iteration] 12500 | [source acc.] 65.62% | [target acc.] 59.38%\n",
      "[iteration] 12500 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5564.56\n",
      "[iteration] 12550 | [source acc.] 50.00% | [target acc.] 65.62%\n",
      "[iteration] 12550 | [d-loss] 0.62 | [t-loss] 0.73 | [r-loss] 4752.74\n",
      "[iteration] 12600 | [source acc.] 9.38% | [target acc.] 93.75%\n",
      "[iteration] 12600 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4884.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration] 12650 | [source acc.] 62.50% | [target acc.] 50.00%\n",
      "[iteration] 12650 | [d-loss] 0.68 | [t-loss] 0.73 | [r-loss] 6327.48\n",
      "[iteration] 12700 | [source acc.] 81.25% | [target acc.] 46.88%\n",
      "[iteration] 12700 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 6149.95\n",
      "[iteration] 12750 | [source acc.] 31.25% | [target acc.] 75.00%\n",
      "[iteration] 12750 | [d-loss] 0.68 | [t-loss] 0.79 | [r-loss] 7590.53\n",
      "[iteration] 12800 | [source acc.] 75.00% | [target acc.] 56.25%\n",
      "[iteration] 12800 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 7676.10\n",
      "[iteration] 12850 | [source acc.] 34.38% | [target acc.] 71.88%\n",
      "[iteration] 12850 | [d-loss] 0.73 | [t-loss] 0.75 | [r-loss] 5996.61\n",
      "[iteration] 12900 | [source acc.] 75.00% | [target acc.] 31.25%\n",
      "[iteration] 12900 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 7823.95\n",
      "[iteration] 12950 | [source acc.] 78.12% | [target acc.] 40.62%\n",
      "[iteration] 12950 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 5917.41\n",
      "[iteration] 13000 | [source acc.] 65.62% | [target acc.] 59.38%\n",
      "[iteration] 13000 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4305.42\n",
      "[iteration] 13050 | [source acc.] 78.12% | [target acc.] 34.38%\n",
      "[iteration] 13050 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 3974.38\n",
      "[iteration] 13100 | [source acc.] 59.38% | [target acc.] 46.88%\n",
      "[iteration] 13100 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 4593.06\n",
      "[iteration] 13150 | [source acc.] 75.00% | [target acc.] 46.88%\n",
      "[iteration] 13150 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 6761.52\n",
      "[iteration] 13200 | [source acc.] 65.62% | [target acc.] 50.00%\n",
      "[iteration] 13200 | [d-loss] 0.59 | [t-loss] 0.75 | [r-loss] 5909.75\n",
      "[iteration] 13250 | [source acc.] 46.88% | [target acc.] 68.75%\n",
      "[iteration] 13250 | [d-loss] 0.58 | [t-loss] 0.75 | [r-loss] 3933.68\n",
      "[iteration] 13300 | [source acc.] 46.88% | [target acc.] 75.00%\n",
      "[iteration] 13300 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4593.87\n",
      "[iteration] 13350 | [source acc.] 53.12% | [target acc.] 71.88%\n",
      "[iteration] 13350 | [d-loss] 0.88 | [t-loss] 0.74 | [r-loss] 5282.40\n",
      "[iteration] 13400 | [source acc.] 56.25% | [target acc.] 75.00%\n",
      "[iteration] 13400 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 6034.51\n",
      "[iteration] 13450 | [source acc.] 50.00% | [target acc.] 71.88%\n",
      "[iteration] 13450 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5278.05\n",
      "[iteration] 13500 | [source acc.] 75.00% | [target acc.] 46.88%\n",
      "[iteration] 13500 | [d-loss] 0.63 | [t-loss] 0.76 | [r-loss] 4847.38\n",
      "[iteration] 13550 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 13550 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 3908.20\n",
      "[iteration] 13600 | [source acc.] 50.00% | [target acc.] 65.62%\n",
      "[iteration] 13600 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 4781.87\n",
      "[iteration] 13650 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 13650 | [d-loss] 0.57 | [t-loss] 0.75 | [r-loss] 6545.83\n",
      "[iteration] 13700 | [source acc.] 75.00% | [target acc.] 56.25%\n",
      "[iteration] 13700 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 5818.49\n",
      "[iteration] 13750 | [source acc.] 62.50% | [target acc.] 62.50%\n",
      "[iteration] 13750 | [d-loss] 0.58 | [t-loss] 0.76 | [r-loss] 5075.65\n",
      "[iteration] 13800 | [source acc.] 84.38% | [target acc.] 40.62%\n",
      "[iteration] 13800 | [d-loss] 0.69 | [t-loss] 0.74 | [r-loss] 7375.24\n",
      "[iteration] 13850 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 13850 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 4855.46\n",
      "[iteration] 13900 | [source acc.] 62.50% | [target acc.] 71.88%\n",
      "[iteration] 13900 | [d-loss] 0.62 | [t-loss] 0.76 | [r-loss] 4895.93\n",
      "[iteration] 13950 | [source acc.] 46.88% | [target acc.] 68.75%\n",
      "[iteration] 13950 | [d-loss] 0.67 | [t-loss] 0.76 | [r-loss] 5355.42\n",
      "[iteration] 14000 | [source acc.] 25.00% | [target acc.] 90.62%\n",
      "[iteration] 14000 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 5707.72\n",
      "[iteration] 14050 | [source acc.] 53.12% | [target acc.] 68.75%\n",
      "[iteration] 14050 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] 4536.43\n",
      "[iteration] 14100 | [source acc.] 75.00% | [target acc.] 40.62%\n",
      "[iteration] 14100 | [d-loss] 0.68 | [t-loss] 0.77 | [r-loss] 5462.52\n",
      "[iteration] 14150 | [source acc.] 56.25% | [target acc.] 62.50%\n",
      "[iteration] 14150 | [d-loss] 0.74 | [t-loss] 0.75 | [r-loss] 3559.19\n",
      "[iteration] 14200 | [source acc.] 78.12% | [target acc.] 43.75%\n",
      "[iteration] 14200 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 3968.10\n",
      "[iteration] 14250 | [source acc.] 65.62% | [target acc.] 65.62%\n",
      "[iteration] 14250 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 4436.50\n",
      "[iteration] 14300 | [source acc.] 43.75% | [target acc.] 56.25%\n",
      "[iteration] 14300 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 8679.34\n",
      "[iteration] 14350 | [source acc.] 65.62% | [target acc.] 56.25%\n",
      "[iteration] 14350 | [d-loss] 0.70 | [t-loss] 0.79 | [r-loss] 5088.74\n",
      "[iteration] 14400 | [source acc.] 90.62% | [target acc.] 56.25%\n",
      "[iteration] 14400 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 3392.21\n",
      "[iteration] 14450 | [source acc.] 71.88% | [target acc.] 59.38%\n",
      "[iteration] 14450 | [d-loss] 0.61 | [t-loss] 0.75 | [r-loss] 5499.25\n",
      "[iteration] 14500 | [source acc.] 78.12% | [target acc.] 59.38%\n",
      "[iteration] 14500 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 4257.65\n",
      "[iteration] 14550 | [source acc.] 56.25% | [target acc.] 65.62%\n",
      "[iteration] 14550 | [d-loss] 0.56 | [t-loss] 0.75 | [r-loss] 4072.51\n",
      "[iteration] 14600 | [source acc.] 71.88% | [target acc.] 31.25%\n",
      "[iteration] 14600 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4653.25\n",
      "[iteration] 14650 | [source acc.] 62.50% | [target acc.] 56.25%\n",
      "[iteration] 14650 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] 7819.75\n",
      "[iteration] 14700 | [source acc.] 40.62% | [target acc.] 75.00%\n",
      "[iteration] 14700 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] 7008.45\n",
      "[iteration] 14750 | [source acc.] 46.88% | [target acc.] 75.00%\n",
      "[iteration] 14750 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 6786.83\n",
      "[iteration] 14800 | [source acc.] 87.50% | [target acc.] 37.50%\n",
      "[iteration] 14800 | [d-loss] 0.60 | [t-loss] 0.75 | [r-loss] 5902.18\n",
      "[iteration] 14850 | [source acc.] 62.50% | [target acc.] 43.75%\n",
      "[iteration] 14850 | [d-loss] 0.62 | [t-loss] 0.76 | [r-loss] 5611.16\n",
      "[iteration] 14900 | [source acc.] 37.50% | [target acc.] 84.38%\n",
      "[iteration] 14900 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] 4027.86\n",
      "[iteration] 14950 | [source acc.] 43.75% | [target acc.] 87.50%\n",
      "[iteration] 14950 | [d-loss] 0.59 | [t-loss] 0.77 | [r-loss] 3121.73\n",
      "[iteration] 15000 | [source acc.] 65.62% | [target acc.] 59.38%\n",
      "[iteration] 15000 | [d-loss] 0.60 | [t-loss] 0.75 | [r-loss] 4746.92\n",
      "[iteration] 15050 | [source acc.] 53.12% | [target acc.] 68.75%\n",
      "[iteration] 15050 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 5896.05\n",
      "[iteration] 15100 | [source acc.] 34.38% | [target acc.] 71.88%\n",
      "[iteration] 15100 | [d-loss] 0.70 | [t-loss] 0.80 | [r-loss] 5274.94\n",
      "[iteration] 15150 | [source acc.] 46.88% | [target acc.] 50.00%\n",
      "[iteration] 15150 | [d-loss] 0.60 | [t-loss] 0.77 | [r-loss] 6842.07\n",
      "[iteration] 15200 | [source acc.] 75.00% | [target acc.] 46.88%\n",
      "[iteration] 15200 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 5714.88\n",
      "[iteration] 15250 | [source acc.] 31.25% | [target acc.] 96.88%\n",
      "[iteration] 15250 | [d-loss] 0.69 | [t-loss] 0.75 | [r-loss] 6586.44\n",
      "[iteration] 15300 | [source acc.] 37.50% | [target acc.] 75.00%\n",
      "[iteration] 15300 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 5493.11\n",
      "[iteration] 15350 | [source acc.] 90.62% | [target acc.] 15.62%\n",
      "[iteration] 15350 | [d-loss] 0.63 | [t-loss] 0.76 | [r-loss] 5440.57\n",
      "[iteration] 15400 | [source acc.] 75.00% | [target acc.] 59.38%\n",
      "[iteration] 15400 | [d-loss] 0.65 | [t-loss] 0.76 | [r-loss] 5184.99\n",
      "[iteration] 15450 | [source acc.] 53.12% | [target acc.] 71.88%\n",
      "[iteration] 15450 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 5541.16\n",
      "[iteration] 15500 | [source acc.] 59.38% | [target acc.] 56.25%\n",
      "[iteration] 15500 | [d-loss] 0.60 | [t-loss] 0.74 | [r-loss] 5235.25\n",
      "[iteration] 15550 | [source acc.] 78.12% | [target acc.] 21.88%\n",
      "[iteration] 15550 | [d-loss] 0.60 | [t-loss] 0.75 | [r-loss] 5795.46\n",
      "[iteration] 15600 | [source acc.] 46.88% | [target acc.] 78.12%\n",
      "[iteration] 15600 | [d-loss] 0.65 | [t-loss] 0.83 | [r-loss] 4213.89\n",
      "[iteration] 15650 | [source acc.] 53.12% | [target acc.] 81.25%\n",
      "[iteration] 15650 | [d-loss] 0.70 | [t-loss] 0.76 | [r-loss] 4360.35\n",
      "[iteration] 15700 | [source acc.] 68.75% | [target acc.] 53.12%\n",
      "[iteration] 15700 | [d-loss] 0.61 | [t-loss] 0.76 | [r-loss] 5060.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration] 15750 | [source acc.] 75.00% | [target acc.] 62.50%\n",
      "[iteration] 15750 | [d-loss] 0.60 | [t-loss] 0.76 | [r-loss] 4501.57\n",
      "[iteration] 15800 | [source acc.] 75.00% | [target acc.] 56.25%\n",
      "[iteration] 15800 | [d-loss] 0.59 | [t-loss] 0.77 | [r-loss] 2573.09\n",
      "[iteration] 15850 | [source acc.] 46.88% | [target acc.] 53.12%\n",
      "[iteration] 15850 | [d-loss] 0.66 | [t-loss] 0.76 | [r-loss] 5339.25\n",
      "[iteration] 15900 | [source acc.] 59.38% | [target acc.] 53.12%\n",
      "[iteration] 15900 | [d-loss] 0.58 | [t-loss] 0.76 | [r-loss] 5039.63\n",
      "[iteration] 15950 | [source acc.] 62.50% | [target acc.] 56.25%\n",
      "[iteration] 15950 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 4978.31\n",
      "[iteration] 16000 | [source acc.] 62.50% | [target acc.] 78.12%\n",
      "[iteration] 16000 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 3709.85\n",
      "[iteration] 16050 | [source acc.] 68.75% | [target acc.] 59.38%\n",
      "[iteration] 16050 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5031.08\n",
      "[iteration] 16100 | [source acc.] 37.50% | [target acc.] 81.25%\n",
      "[iteration] 16100 | [d-loss] 0.58 | [t-loss] 0.76 | [r-loss] 7298.33\n",
      "[iteration] 16150 | [source acc.] 62.50% | [target acc.] 87.50%\n",
      "[iteration] 16150 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 5997.59\n",
      "[iteration] 16200 | [source acc.] 59.38% | [target acc.] 62.50%\n",
      "[iteration] 16200 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 3666.25\n",
      "[iteration] 16250 | [source acc.] 59.38% | [target acc.] 65.62%\n",
      "[iteration] 16250 | [d-loss] 0.69 | [t-loss] 0.78 | [r-loss] 5155.52\n",
      "[iteration] 16300 | [source acc.] 68.75% | [target acc.] 59.38%\n",
      "[iteration] 16300 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 4870.31\n",
      "[iteration] 16350 | [source acc.] 21.88% | [target acc.] 87.50%\n",
      "[iteration] 16350 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 5444.99\n",
      "[iteration] 16400 | [source acc.] 53.12% | [target acc.] 71.88%\n",
      "[iteration] 16400 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 3903.86\n",
      "[iteration] 16450 | [source acc.] 75.00% | [target acc.] 50.00%\n",
      "[iteration] 16450 | [d-loss] 0.59 | [t-loss] 0.74 | [r-loss] 4947.08\n",
      "[iteration] 16500 | [source acc.] 65.62% | [target acc.] 53.12%\n",
      "[iteration] 16500 | [d-loss] 0.67 | [t-loss] 0.74 | [r-loss] 4017.05\n",
      "[iteration] 16550 | [source acc.] 56.25% | [target acc.] 68.75%\n",
      "[iteration] 16550 | [d-loss] 0.68 | [t-loss] 0.75 | [r-loss] 5076.82\n",
      "[iteration] 16600 | [source acc.] 62.50% | [target acc.] 53.12%\n",
      "[iteration] 16600 | [d-loss] 0.74 | [t-loss] 0.77 | [r-loss] 6521.82\n",
      "[iteration] 16650 | [source acc.] 53.12% | [target acc.] 87.50%\n",
      "[iteration] 16650 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 5896.95\n",
      "[iteration] 16700 | [source acc.] 53.12% | [target acc.] 71.88%\n",
      "[iteration] 16700 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] 4673.19\n",
      "[iteration] 16750 | [source acc.] 62.50% | [target acc.] 71.88%\n",
      "[iteration] 16750 | [d-loss] 0.71 | [t-loss] 0.74 | [r-loss] 6781.92\n",
      "[iteration] 16800 | [source acc.] 65.62% | [target acc.] 65.62%\n",
      "[iteration] 16800 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4418.81\n",
      "[iteration] 16850 | [source acc.] 84.38% | [target acc.] 40.62%\n",
      "[iteration] 16850 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 4536.66\n",
      "[iteration] 16900 | [source acc.] 50.00% | [target acc.] 59.38%\n",
      "[iteration] 16900 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 4108.11\n",
      "[iteration] 16950 | [source acc.] 68.75% | [target acc.] 46.88%\n",
      "[iteration] 16950 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 6758.41\n",
      "[iteration] 17000 | [source acc.] 40.62% | [target acc.] 81.25%\n",
      "[iteration] 17000 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 4318.04\n",
      "[iteration] 17050 | [source acc.] 68.75% | [target acc.] 62.50%\n",
      "[iteration] 17050 | [d-loss] 0.59 | [t-loss] 0.76 | [r-loss] 3443.72\n",
      "[iteration] 17100 | [source acc.] 71.88% | [target acc.] 37.50%\n",
      "[iteration] 17100 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 3662.36\n",
      "[iteration] 17150 | [source acc.] 59.38% | [target acc.] 50.00%\n",
      "[iteration] 17150 | [d-loss] 0.69 | [t-loss] 0.76 | [r-loss] 7498.94\n",
      "[iteration] 17200 | [source acc.] 18.75% | [target acc.] 87.50%\n",
      "[iteration] 17200 | [d-loss] 0.61 | [t-loss] 0.74 | [r-loss] 4890.54\n",
      "[iteration] 17250 | [source acc.] 68.75% | [target acc.] 50.00%\n",
      "[iteration] 17250 | [d-loss] 0.66 | [t-loss] 0.77 | [r-loss] 3860.77\n",
      "[iteration] 17300 | [source acc.] 59.38% | [target acc.] 50.00%\n",
      "[iteration] 17300 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 5016.42\n",
      "[iteration] 17350 | [source acc.] 21.88% | [target acc.] 84.38%\n",
      "[iteration] 17350 | [d-loss] 0.66 | [t-loss] 0.73 | [r-loss] 6319.06\n",
      "[iteration] 17400 | [source acc.] 71.88% | [target acc.] 59.38%\n",
      "[iteration] 17400 | [d-loss] 0.64 | [t-loss] 0.76 | [r-loss] 4830.21\n",
      "[iteration] 17450 | [source acc.] 90.62% | [target acc.] 6.25%\n",
      "[iteration] 17450 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 3712.54\n",
      "[iteration] 17500 | [source acc.] 75.00% | [target acc.] 18.75%\n",
      "[iteration] 17500 | [d-loss] 0.57 | [t-loss] 0.75 | [r-loss] 6022.06\n",
      "[iteration] 17550 | [source acc.] 46.88% | [target acc.] 59.38%\n",
      "[iteration] 17550 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 4752.82\n",
      "[iteration] 17600 | [source acc.] 96.88% | [target acc.] 3.12%\n",
      "[iteration] 17600 | [d-loss] 0.62 | [t-loss] 0.76 | [r-loss] 6224.39\n",
      "[iteration] 17650 | [source acc.] 56.25% | [target acc.] 84.38%\n",
      "[iteration] 17650 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4825.29\n",
      "[iteration] 17700 | [source acc.] 68.75% | [target acc.] 75.00%\n",
      "[iteration] 17700 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 4832.16\n",
      "[iteration] 17750 | [source acc.] 34.38% | [target acc.] 84.38%\n",
      "[iteration] 17750 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 6162.79\n",
      "[iteration] 17800 | [source acc.] 71.88% | [target acc.] 46.88%\n",
      "[iteration] 17800 | [d-loss] 0.58 | [t-loss] 0.74 | [r-loss] 4516.66\n",
      "[iteration] 17850 | [source acc.] 56.25% | [target acc.] 62.50%\n",
      "[iteration] 17850 | [d-loss] 0.68 | [t-loss] 0.76 | [r-loss] 6368.90\n",
      "[iteration] 17900 | [source acc.] 78.12% | [target acc.] 34.38%\n",
      "[iteration] 17900 | [d-loss] 0.66 | [t-loss] 0.74 | [r-loss] 5764.78\n",
      "[iteration] 17950 | [source acc.] 56.25% | [target acc.] 71.88%\n",
      "[iteration] 17950 | [d-loss] 0.70 | [t-loss] 0.76 | [r-loss] 5999.83\n",
      "[iteration] 18000 | [source acc.] 53.12% | [target acc.] 90.62%\n",
      "[iteration] 18000 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 5200.47\n",
      "[iteration] 18050 | [source acc.] 65.62% | [target acc.] 59.38%\n",
      "[iteration] 18050 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 7856.58\n",
      "[iteration] 18100 | [source acc.] 31.25% | [target acc.] 81.25%\n",
      "[iteration] 18100 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 3338.87\n",
      "[iteration] 18150 | [source acc.] 96.88% | [target acc.] 28.12%\n",
      "[iteration] 18150 | [d-loss] 0.62 | [t-loss] 0.75 | [r-loss] 4211.20\n",
      "[iteration] 18200 | [source acc.] 62.50% | [target acc.] 68.75%\n",
      "[iteration] 18200 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] 6119.34\n",
      "[iteration] 18250 | [source acc.] 62.50% | [target acc.] 62.50%\n",
      "[iteration] 18250 | [d-loss] 0.67 | [t-loss] 0.73 | [r-loss] 6023.71\n",
      "[iteration] 18300 | [source acc.] 78.12% | [target acc.] 46.88%\n",
      "[iteration] 18300 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4592.98\n",
      "[iteration] 18350 | [source acc.] 65.62% | [target acc.] 78.12%\n",
      "[iteration] 18350 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 6769.09\n",
      "[iteration] 18400 | [source acc.] 37.50% | [target acc.] 81.25%\n",
      "[iteration] 18400 | [d-loss] 0.61 | [t-loss] 0.75 | [r-loss] 3453.08\n",
      "[iteration] 18450 | [source acc.] 75.00% | [target acc.] 50.00%\n",
      "[iteration] 18450 | [d-loss] 0.64 | [t-loss] 0.77 | [r-loss] 4107.46\n",
      "[iteration] 18500 | [source acc.] 71.88% | [target acc.] 50.00%\n",
      "[iteration] 18500 | [d-loss] 0.65 | [t-loss] 0.74 | [r-loss] 5406.00\n",
      "[iteration] 18550 | [source acc.] 62.50% | [target acc.] 43.75%\n",
      "[iteration] 18550 | [d-loss] 0.59 | [t-loss] 0.75 | [r-loss] 5697.21\n",
      "[iteration] 18600 | [source acc.] 34.38% | [target acc.] 71.88%\n",
      "[iteration] 18600 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4421.93\n",
      "[iteration] 18650 | [source acc.] 50.00% | [target acc.] 71.88%\n",
      "[iteration] 18650 | [d-loss] 0.67 | [t-loss] 0.75 | [r-loss] 4900.50\n",
      "[iteration] 18700 | [source acc.] 53.12% | [target acc.] 46.88%\n",
      "[iteration] 18700 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 5122.62\n",
      "[iteration] 18750 | [source acc.] 50.00% | [target acc.] 59.38%\n",
      "[iteration] 18750 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 6369.74\n",
      "[iteration] 18800 | [source acc.] 68.75% | [target acc.] 40.62%\n",
      "[iteration] 18800 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 6453.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration] 18850 | [source acc.] 34.38% | [target acc.] 59.38%\n",
      "[iteration] 18850 | [d-loss] 0.71 | [t-loss] 0.75 | [r-loss] 5150.20\n",
      "[iteration] 18900 | [source acc.] 46.88% | [target acc.] 56.25%\n",
      "[iteration] 18900 | [d-loss] 0.68 | [t-loss] 0.73 | [r-loss] 5034.47\n",
      "[iteration] 18950 | [source acc.] 34.38% | [target acc.] 90.62%\n",
      "[iteration] 18950 | [d-loss] 0.78 | [t-loss] 0.76 | [r-loss] 6251.14\n",
      "[iteration] 19000 | [source acc.] 81.25% | [target acc.] 53.12%\n",
      "[iteration] 19000 | [d-loss] 0.60 | [t-loss] 0.74 | [r-loss] 4268.25\n",
      "[iteration] 19050 | [source acc.] 59.38% | [target acc.] 56.25%\n",
      "[iteration] 19050 | [d-loss] 0.60 | [t-loss] 0.76 | [r-loss] 4740.73\n",
      "[iteration] 19100 | [source acc.] 65.62% | [target acc.] 56.25%\n",
      "[iteration] 19100 | [d-loss] 0.63 | [t-loss] 0.74 | [r-loss] 3954.05\n",
      "[iteration] 19150 | [source acc.] 65.62% | [target acc.] 65.62%\n",
      "[iteration] 19150 | [d-loss] 0.61 | [t-loss] 0.76 | [r-loss] 6080.97\n",
      "[iteration] 19200 | [source acc.] 43.75% | [target acc.] 56.25%\n",
      "[iteration] 19200 | [d-loss] 0.63 | [t-loss] 0.75 | [r-loss] 5306.00\n",
      "[iteration] 19250 | [source acc.] 71.88% | [target acc.] 40.62%\n",
      "[iteration] 19250 | [d-loss] 0.65 | [t-loss] 0.76 | [r-loss] 3899.92\n",
      "[iteration] 19300 | [source acc.] 87.50% | [target acc.] 18.75%\n",
      "[iteration] 19300 | [d-loss] 0.62 | [t-loss] 0.73 | [r-loss] 5262.40\n",
      "[iteration] 19350 | [source acc.] 31.25% | [target acc.] 84.38%\n",
      "[iteration] 19350 | [d-loss] 0.65 | [t-loss] 0.76 | [r-loss] 7130.94\n",
      "[iteration] 19400 | [source acc.] 56.25% | [target acc.] 75.00%\n",
      "[iteration] 19400 | [d-loss] 0.65 | [t-loss] 0.76 | [r-loss] 3673.77\n",
      "[iteration] 19450 | [source acc.] 87.50% | [target acc.] 59.38%\n",
      "[iteration] 19450 | [d-loss] 0.70 | [t-loss] 0.75 | [r-loss] 4687.17\n",
      "[iteration] 19500 | [source acc.] 81.25% | [target acc.] 59.38%\n",
      "[iteration] 19500 | [d-loss] 0.64 | [t-loss] 0.75 | [r-loss] 4466.41\n",
      "[iteration] 19550 | [source acc.] 40.62% | [target acc.] 87.50%\n",
      "[iteration] 19550 | [d-loss] 0.64 | [t-loss] 0.74 | [r-loss] 3813.36\n",
      "[iteration] 19600 | [source acc.] 50.00% | [target acc.] 65.62%\n",
      "[iteration] 19600 | [d-loss] 0.68 | [t-loss] 0.74 | [r-loss] 4612.94\n",
      "[iteration] 19650 | [source acc.] 62.50% | [target acc.] 62.50%\n",
      "[iteration] 19650 | [d-loss] 0.69 | [t-loss] 0.76 | [r-loss] 4378.22\n",
      "[iteration] 19700 | [source acc.] 65.62% | [target acc.] 75.00%\n",
      "[iteration] 19700 | [d-loss] 0.61 | [t-loss] 0.75 | [r-loss] 3782.79\n",
      "[iteration] 19750 | [source acc.] 0.00% | [target acc.] 100.00%\n",
      "[iteration] 19750 | [d-loss] 0.71 | [t-loss] 0.83 | [r-loss] 6475.90\n",
      "[iteration] 19800 | [source acc.] 56.25% | [target acc.] 68.75%\n",
      "[iteration] 19800 | [d-loss] 0.69 | [t-loss] 0.77 | [r-loss] 4658.29\n",
      "[iteration] 19850 | [source acc.] 53.12% | [target acc.] 59.38%\n",
      "[iteration] 19850 | [d-loss] 0.71 | [t-loss] 0.76 | [r-loss] 8416.57\n",
      "[iteration] 19900 | [source acc.] 59.38% | [target acc.] 65.62%\n",
      "[iteration] 19900 | [d-loss] 0.65 | [t-loss] 0.75 | [r-loss] 4419.27\n",
      "[iteration] 19950 | [source acc.] 37.50% | [target acc.] 75.00%\n",
      "[iteration] 19950 | [d-loss] 0.66 | [t-loss] 0.75 | [r-loss] 4276.00\n"
     ]
    }
   ],
   "source": [
    "X_source = X_train.values\n",
    "Y_source = y_train.values\n",
    "X_target = X_test.values\n",
    "\n",
    "num_iterations = 20000\n",
    "transformer, regressor, discriminator, log = train_trd(X_source, Y_source, X_target,[9,8,10,11],\n",
    "                                                       embedding_dim=5, feature_dim=20,\n",
    "                                                       lr_list=[3e-4,3e-4,5e-4],\n",
    "                                                       num_iterations=num_iterations,\n",
    "                                                       cycle_length=2000, split=200,\n",
    "                                                       iter_l=500, iter_s=5,\n",
    "                                                       batch_size=32, display_num=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:24:36.362602Z",
     "start_time": "2020-03-24T06:24:36.120251Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'T/D Loss')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('font',family='Times New Roman')\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.array(log)[:,0])\n",
    "ax.plot(np.array(log)[:,1])\n",
    "plt.xlim([0, num_iterations])\n",
    "plt.ylabel('T/D Loss')\n",
    "\n",
    "ax1 = ax.twinx()\n",
    "ax1.plot(np.arange(5000,num_iterations),np.sqrt(np.array(log)[5000:,2]),c='g')\n",
    "plt.ylabel('R Loss')\n",
    "plt.ylim([30,120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:18:40.687768Z",
     "start_time": "2020-03-24T06:18:38.482665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRD] mae: 77.38 | mape: 61.29% | mspe: 36.02%\n"
     ]
    }
   ],
   "source": [
    "feat_test = transformer.predict([X_test[numeric_features],\n",
    "                                 X_test['interval'],\n",
    "                                 X_test['weekday'],\n",
    "                                 X_test['holiday'],\n",
    "                                 X_test['peak']])\n",
    "y_pred = regressor.predict(feat_test).reshape(-1)\n",
    "\n",
    "print(\"[TRD] mae: {:.2f} | mape: {:.2f}% | mspe: {:.2f}%\".format(\n",
    "    mae(y_pred, y_test),\n",
    "    100 * mape(y_pred, y_test), 100 * mspe(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:18:50.142484Z",
     "start_time": "2020-03-24T06:18:46.776485Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_train = transformer.predict([X_train[numeric_features],\n",
    "                                  X_train['interval'],\n",
    "                                  X_train['weekday'],\n",
    "                                  X_train['holiday'],\n",
    "                                  X_train['peak']])\n",
    "feat = np.concatenate((feat_train, feat_test),axis=0)\n",
    "np.save('../Data/invariant_feature_dann.npy',feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T09:20:24.581687Z",
     "start_time": "2020-03-23T09:19:54.407379Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer.save('../Data/transformer.h5')\n",
    "discriminator.save('../Data/discriminator.h5')\n",
    "regressor.save('../Data/original_regressor.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regressor from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:29:50.323991Z",
     "start_time": "2020-03-24T06:29:40.758570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=10,weights='distance',n_jobs=-1)\n",
    "knn = knn.fit(feat_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:29:53.787725Z",
     "start_time": "2020-03-24T06:29:52.738532Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kNN] mae: 69.05 | mape: 62.09% | mspe: 37.06%\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(feat_test)\n",
    "print(\"[kNN] mae: {:.2f} | mape: {:.2f}% | mspe: {:.2f}%\".format(\n",
    "    mae(y_pred, y_test),\n",
    "    100 * mape(y_pred,y_test), 100 * mspe(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:31:33.830189Z",
     "start_time": "2020-03-24T06:31:33.722475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_new_train = np.concatenate((feat_train,X_train.values),axis=1)\n",
    "X_new_test = np.concatenate((feat_test,X_test.values),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T06:35:58.590151Z",
     "start_time": "2020-03-24T06:31:44.590412Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] mae: 55.76 | mape: 52.43% | mspe: 27.85%\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_new_train, y_train)\n",
    "\n",
    "params = {\n",
    "    'objective':'regression',\n",
    "    'boosting':'gbdt',\n",
    "    'num_rounds':10000,\n",
    "    'learning_rate':0.01,\n",
    "    'max_depth':8,\n",
    "    'num_leaves':80,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':50,\n",
    "    'verbose':2\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params, train_data)\n",
    "\n",
    "y_pred = gbm.predict(X_new_test)\n",
    "\n",
    "print(\"[LightGBM] mae: {:.2f} | mape: {:.2f}% | mspe: {:.2f}%\".format(\n",
    "    mae(y_pred, y_test),\n",
    "    100 * mape(y_pred,y_test), 100 * mspe(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T09:44:21.553852Z",
     "start_time": "2020-03-23T09:44:21.547869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_regressor(input_shape, num_blocks):\n",
    "    x_in = Input(shape=(input_shape,))\n",
    "    h = Reshape((input_shapet, 1))(x_in)\n",
    "    \n",
    "    h = Conv1D(32,kernel_size=10,)\n",
    "    def build_resblock(h):\n",
    "        h1 = build_dense(h, 512, drop=False, norm=False)\n",
    "        h2 = build_dense(h1, 256, drop=False, norm=False)\n",
    "        h3 = build_dense(h2, 512, drop=False, norm=False)\n",
    "        h4 = Add()([h1,h3])\n",
    "        return h4\n",
    "    \n",
    "    h = build_dense(x_in, 128, drop=False, norm=False)\n",
    "    for i in range(num_blocks):\n",
    "        h = build_resblock(h)\n",
    "        \n",
    "    o = build_dense(h, 1, drop=False, norm=False)\n",
    "    \n",
    "    reg = Model(inputs=x_in, outputs=o)\n",
    "    return reg"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
