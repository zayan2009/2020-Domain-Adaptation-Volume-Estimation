{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Reshape, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Add, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import he_normal, constant\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical, normalize\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:33:54.915443Z",
     "start_time": "2020-03-24T03:33:54.538451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avi_train_seg = np.load('../Data/avi_train_seg.npy')\n",
    "trj_train_seg = np.load('../Data/trj_train_seg.npy')\n",
    "lane_functions = np.load('../Data/lane_functions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:33:54.937384Z",
     "start_time": "2020-03-24T03:33:54.926414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_lanes = lane_functions.sum(axis=1)\n",
    "lane_functions = pd.DataFrame(lane_functions,columns=['through','left','right',\n",
    "                                                      'thr_left','thr_right','u_turn'])\n",
    "lane_functions['linkIdx'] = np.arange(1,25)\n",
    "lane_functions['num_lanes'] = num_lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap(x):\n",
    "    base = x['through'] * 1 + x['left'] * 0.85 + x['right'] * 1\n",
    "    mix = x['thr_left'] * 0.8 + x['thr_right'] * 1 + x['u_turn'] * 0.7\n",
    "    return base + mix\n",
    "\n",
    "lane_functions['capacity'] = lane_functions.apply(lambda x:cap(x),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:33:56.925066Z",
     "start_time": "2020-03-24T03:33:56.917088Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sample(trj_train_seg):\n",
    "    data = pd.DataFrame(columns=['linkIdx','datetime','volume'])\n",
    "    for approach in range(24):\n",
    "        table = pd.DataFrame(np.zeros((30 * 144,3)),columns=['linkIdx','datetime','volume'])\n",
    "        table.iloc[:,0] = approach + 1\n",
    "        for day in range(30):\n",
    "            if day < 9:\n",
    "                datetime = [pd.to_datetime('2018010' + str(day + 1)) + pd.Timedelta(i * 10,unit='m') for i in range(144)]\n",
    "            else:\n",
    "                datetime = [pd.to_datetime('201801' + str(day + 1)) + pd.Timedelta(i * 10,unit='m') for i in range(144)]\n",
    "            table.iloc[144 * day:144 * (day + 1),2] = trj_train_seg[day,:,approach]\n",
    "            table.iloc[144 * day:144 * (day + 1),1] = datetime\n",
    "        data = pd.concat((data,table))\n",
    "    data['linkIdx'] = data['linkIdx'].astype('int')\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    data['volume'] = data['volume'].astype('float')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:13:37.640732Z",
     "start_time": "2020-03-24T05:13:09.293567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Basic Stats] num. of samples: 103680\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_sample(trj_train_seg), get_sample(avi_train_seg)\n",
    "print(\"[Basic Stats] num. of samples: {:d}\".format(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T03:34:26.527900Z",
     "start_time": "2020-03-24T03:34:26.510946Z"
    }
   },
   "outputs": [],
   "source": [
    "def holiday(x):\n",
    "    if x.month == 1 and x.day == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def peak(x):\n",
    "    if 7 < x.hour < 9:\n",
    "        return 1\n",
    "    elif 11 < x.hour < 13:\n",
    "        return 2\n",
    "    elif 17 < x.hour < 19:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def itv_cnt(x, itv_length=600):\n",
    "    return (x.hour * 3600 + x.minute * 60) // itv_length\n",
    "\n",
    "\n",
    "def exponential_smoothing(alpha, s):\n",
    "    s2 = np.zeros(s.shape)\n",
    "    s2[0] = s[0]\n",
    "    for i in range(1, len(s2)):\n",
    "        s2[i] = alpha * s[i] + (1 - alpha) * s2[i - 1]\n",
    "    return s2\n",
    "\n",
    "\n",
    "def get_es_volume(trj_train_seg, alpha):\n",
    "    trj_train_seg_es = trj_train_seg.copy()\n",
    "    for day_idx in range(30):\n",
    "        for seg_idx in range(24):\n",
    "            seq = trj_train_seg[day_idx, :, seg_idx]\n",
    "            seq_es = exponential_smoothing(alpha, seq)\n",
    "            trj_train_seg_es[day_idx, :, seg_idx] = seq_es\n",
    "    return get_sample(trj_train_seg_es)['volume']\n",
    "\n",
    "\n",
    "def merge_volume_features(X, feature_cols, target_col, aggfuncs=['mean', 'median', 'std']):\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        for fn in aggfuncs:\n",
    "            df = X.pivot_table(index='linkIdx',\n",
    "                               columns=feature,\n",
    "                               values=target_col,\n",
    "                               aggfunc=fn).reset_index()\n",
    "            df.columns = ['linkIdx'] + list(df.columns[1:])\n",
    "            df = df.melt(id_vars=['linkIdx'],\n",
    "                         value_vars=list(df.columns[1:]),\n",
    "                         var_name=feature,\n",
    "                         value_name=feature + '_' + fn + '_' + target_col)\n",
    "            df[feature] = pd.to_numeric(df[feature])\n",
    "            X = pd.merge(X, df, on=['linkIdx',feature])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:10.620296Z",
     "start_time": "2020-03-24T05:13:43.534762Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Feature Engineering\n",
    "# =============================\n",
    "\n",
    "# static attributes\n",
    "X = pd.merge(X,lane_functions)\n",
    "X['weekday'] = X['datetime'].map(lambda x:x.weekday())\n",
    "X['interval'] = X['datetime'].map(lambda x:itv_cnt(x))\n",
    "X['holiday'] = X['datetime'].map(lambda x:holiday(x))\n",
    "X['peak'] = X['datetime'].map(lambda x:peak(x))\n",
    "X['linkIdx'] = X['linkIdx'].astype(int)\n",
    "\n",
    "# exponenrially smoothed volume\n",
    "X['volume_es_p7'] = get_es_volume(trj_train_seg, alpha=0.7).values\n",
    "X['volume_es_p6'] = get_es_volume(trj_train_seg, alpha=0.6).values\n",
    "X['volume_es_p5'] = get_es_volume(trj_train_seg, alpha=0.5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:10.620296Z",
     "start_time": "2020-03-24T05:13:43.534762Z"
    }
   },
   "outputs": [],
   "source": [
    "# cross volume features\n",
    "feature_cols = ['through', 'left', 'right', 'thr_left',\n",
    "                'thr_right', 'u_turn', 'num_lanes',\n",
    "                'weekday', 'interval', 'holiday', 'peak']\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume')\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume_es_p7')\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume_es_p6')\n",
    "X = merge_volume_features(X, feature_cols, target_col='volume_es_p5')\n",
    "\n",
    "# penetration rates\n",
    "X['tmp'] = Y['volume'].values\n",
    "interval_volume = X.pivot_table(index='interval',\n",
    "                                values=['volume', 'tmp', 'volume_es_p7',\n",
    "                                        'volume_es_p6', 'volume_es_p5'],\n",
    "                                aggfunc='sum').reset_index()\n",
    "interval_volume['penetration'] = interval_volume['volume'] / interval_volume['tmp']\n",
    "interval_volume['penetration_p7'] = interval_volume['volume_es_p7'] / interval_volume['tmp']\n",
    "interval_volume['penetration_p6'] = interval_volume['volume_es_p6'] / interval_volume['tmp']\n",
    "interval_volume['penetration_p5'] = interval_volume['volume_es_p5'] / interval_volume['tmp']\n",
    "interval_volume.drop(['tmp','volume','volume_es_p7','volume_es_p5','volume_es_p6'],axis=1,inplace=True)\n",
    "X = pd.merge(X, interval_volume, on='interval')\n",
    "\n",
    "# scaled volume\n",
    "for up_idx, up_col in enumerate(['volume','volume_es_p7',\n",
    "                                 'volume_es_p6','volume_es_p5']):\n",
    "    for down_idx, down_col in enumerate(['penetration','penetration_p7',\n",
    "                                         'penetration_p6','penetration_p5']):\n",
    "        X['scaled_volume_' + str(up_idx) + '_' + str(down_idx)] = X[up_col] / X[down_col]\n",
    "\n",
    "# correlations & feature count\n",
    "X.drop('tmp',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T05:14:10.620296Z",
     "start_time": "2020-03-24T05:13:43.534762Z"
    }
   },
   "outputs": [],
   "source": [
    "# cat & num features\n",
    "categorical_features = ['interval', 'weekday', 'holiday', 'peak']\n",
    "numeric_features = list(set(X.drop(['datetime', 'linkIdx'], axis=1).columns) - set(categorical_features))\n",
    "print(\"[Basic Stats] dim. of features: {:d}\".format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "X.to_csv('../Data/X.csv')\n",
    "Y.to_csv('../Data/Y.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
